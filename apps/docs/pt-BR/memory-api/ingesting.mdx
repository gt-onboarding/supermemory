---
title: "Ingestão de documents e dados"
sidebarTitle: "Guia de ingestão de conteúdo"
description: "Guia completo para ingestir texto, URLs, arquivos e vários tipos de conteúdo no Supermemory"
---

Supermemory oferece um sistema de ingestão poderoso e flexível que pode processar praticamente qualquer tipo de conteúdo. Seja ao adicionar notas de texto simples, páginas da web, PDFs, imagens ou documents complexos de várias plataformas, nossa API lida com tudo sem complicações.

<div id="understanding-the-mental-model">
  ## Entendendo o modelo mental
</div>

Antes de mergulhar na API, é importante entender como o Supermemory processa seu conteúdo:

<div id="documents-vs-memories">
  ### Documents vs Memórias
</div>

* **Documents**: Tudo o que você coloca no Supermemory (arquivos, URLs, texto) é considerado um **document**
* **Memories**: Os documents são automaticamente divididos em partes menores e pesquisáveis chamadas **memórias**

Quando você usa o endpoint &quot;Add Memory&quot;, na verdade está adicionando um **document**. O trabalho do Supermemory é dividir esse document de forma inteligente em **memórias** ideais que podem ser pesquisadas e recuperadas.

```
Seu conteúdo → Documento → Processamento → Várias memórias
     ↓             ↓           ↓            ↓
   Arquivo PDF → Documento armazenado → Segmentação em chunks → Memórias pesquisáveis
```

Você pode visualizar esse processo no [Supermemory Console](https://console.supermemory.ai), onde verá uma visualização em grafo mostrando como seus documents são divididos em memórias interconectadas.

<div id="content-sources">
  ### Fontes de conteúdo
</div>

O Supermemory aceita conteúdo por três métodos principais:

1. **API direta**: Faça upload de arquivos ou envie conteúdo por endpoints da API
2. **Conectores**: Integrações automatizadas com plataformas como Google Drive, Notion e OneDrive ([saiba mais sobre conectores](/pt-BR/connectors))
3. **Processamento de URL**: Extração automática de páginas da web, vídeos e redes sociais

<div id="overview">
  ## Visão geral
</div>

O sistema de ingestão consiste em vários componentes principais:

* **Vários métodos de entrada**: conteúdo JSON, upload de arquivos e processamento de URLs
* **Processamento assíncrono**: fluxos de trabalho em segundo plano realizam a extração de conteúdo e a segmentação em chunks
* **Detecção automática de conteúdo**: identifica e processa automaticamente diferentes tipos de conteúdo
* **Organização por espaços**: tags de contêiner agrupam memórias relacionadas para melhorar a inferência de contexto
* **Acompanhamento de status**: atualizações de status em tempo real ao longo do pipeline de processamento

<div id="how-it-works">
  ### Como funciona
</div>

<Steps>
  <Step title="Submit Document">
    Envie seu conteúdo (texto, arquivo ou url) para criar um novo documento
  </Step>

  <Step title="Validation">
    A API valida a requisição e verifica limites de taxa/cotas
  </Step>

  <Step title="Document Storage">
    Seu conteúdo é armazenado como documento e colocado em fila para processamento
  </Step>

  <Step title="Content Extraction">
    Extratores especializados processam o documento conforme seu tipo
  </Step>

  <Step title="Memory Creation">
    O documento é dividido de forma inteligente em várias memórias pesquisáveis
  </Step>

  <Step title="Embedding & Indexing">
    As memórias são convertidas em embeddings vetoriais e passam a ser pesquisáveis
  </Step>
</Steps>

<div id="ingestion-endpoints">
  ## Endpoints de ingestão
</div>

<div id="add-document-json-content">
  ### Adicionar Document - Conteúdo JSON
</div>

O endpoint principal para adicionar conteúdo que será processado em documents.

**Endpoint:** `POST /v3/documents`

<Note>
  Apesar do nome do endpoint, você está criando um **document** que o Supermemory fragmentará automaticamente em **memórias** pesquisáveis.
</Note>

<CodeGroup>
  ```bash cURL
  curl https://api.supermemory.ai/v3/documents \
    -H "Authorization: Bearer $SUPERMEMORY_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
      "content": "Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without explicit programming.",
      "containerTags": ["ai-research", "user_123"],
      "metadata": {
        "source": "research-notes",
        "category": "education",
        "priority": "high"
      },
      "customId": "ml-basics-001"
    }'
  ```

  ```typescript TypeScript
  import Supermemory from 'supermemory'

  const client = new Supermemory({
    apiKey: process.env.SUPERMEMORY_API_KEY
  })

  async function addContent() {
      const result = await client.memories.add({
          content: "Machine learning is a subset of artificial intelligence...",
          containerTags: ["ai-research"],
          metadata: {
            source: "research-notes",
            category: "education",
            priority: "high"
          },
          customId: "ml-basics-001"
        })

        console.log(result) // { id: "abc123", status: "queued" }
  }

   addContent()
  ```

  ```python Python
  from supermemory import Supermemory
  import os

  client = Supermemory(api_key=os.environ.get("SUPERMEMORY_API_KEY"))

  result = client.memories.add(
      content="Machine learning is a subset of artificial intelligence...",
      container_tags=["ai-research"],
      metadata={
          "source": "research-notes",
          "category": "education",
          "priority": "high"
      },
      custom_id="ml-basics-001"
  )

  print(result)  # { "id": "abc123", "status": "queued" }
  ```
</CodeGroup>

<div id="request-parameters">
  #### Parâmetros da requisição
</div>

| Parâmetro | Tipo | Obrigatório | Descrição |
|-----------|------|-------------|-----------|
| `content` | string | Sim | Conteúdo a ser processado em um documento. Pode ser texto, url ou outros formatos compatíveis |
| `containerTag` | string | Não | **Recomendado**: Tag única para agrupar memórias relacionadas em um espaço. Padrão: `"sm_project_default"` |
| `containerTags` | string[] | Não | Formato de array legado. Use `containerTag` em vez disso para melhor desempenho |
| `metadata` | object | Não | metadata adicional de pares chave–valor (apenas strings, números e booleanos) |
| `customId` | string | Não | Seu próprio identificador para este documento (máx. 255 caracteres) |
| `raw` | string | Não | Conteúdo bruto para armazenar junto ao conteúdo processado |

<div id="response">
  #### Resposta
</div>

Ao criar um documento com sucesso, você receberá uma confirmação simples com o ID do documento e seu status inicial de processamento:

```json
{
  "id": "D2Ar7Vo7ub83w3PRPZcaP1",
  "status": "queued"
}
```

**O que isso significa:**

* `id`: O identificador único do seu documento — guarde-o para acompanhar o processamento ou consultá-lo depois
* `status`: Estado atual do processamento. `"queued"` significa que está aguardando para ser processado em memórias

<Note>
  O documento começa a ser processado imediatamente em segundo plano. Em questão de segundos a minutos (dependendo do tamanho do conteúdo), ele será dividido em memórias pesquisáveis.
</Note>

<div id="file-upload-drop-and-process">
  ### Upload de arquivo: arraste e processe
</div>

Tem um PDF, imagem ou vídeo? Envie-o diretamente e deixe o Supermemory extrair automaticamente o conteúdo relevante.

**Endpoint:** `POST /v3/documents/file`

**Por que isso é poderoso:** Em vez de copiar manualmente textos de PDFs ou transcrever vídeos, basta enviar o arquivo. O Supermemory faz OCR em imagens, transcrição de vídeos e extração inteligente de texto em documentos.

<CodeGroup>
  ```bash cURL
  curl https://api.supermemory.ai/v3/documents/file \
    -H "Authorization: Bearer $SUPERMEMORY_API_KEY" \
    -F "file=@document.pdf" \
    -F "containerTags=research_project"

  # Resposta:
  # {
  #   "id": "Mx7fK9pL2qR5tE8yU4nC7",
  #   "status": "processing"
  # }
  ```

  ```typescript TypeScript
  import Supermemory from 'supermemory'
  import fs from 'fs'

  const client = new Supermemory({
    apiKey: process.env.SUPERMEMORY_API_KEY
  })

  // Método 1: Usando o método uploadFile do SDK (RECOMENDADO)
  const result = await client.memories.uploadFile({
    file: fs.createReadStream('/path/to/document.pdf'),
    containerTags: 'research_project'  // String, não array!
  })

  // Método 2: Usando fetch com form data (para implementação no navegador/manual)
  const formData = new FormData()
  formData.append('file', fileInput.files[0])
  formData.append('containerTags', 'research_project')

  const response = await fetch('https://api.supermemory.ai/v3/documents/file', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.SUPERMEMORY_API_KEY}`
    },
    body: formData
  })

  const result = await response.json()
  console.log(result)
  // Saída: { id: "Mx7fK9pL2qR5tE8yU4nC7", status: "processing" }
  ```

  ```python Python
  from supermemory import Supermemory

  client = Supermemory(api_key="your_api_key")

  # Método 1: Usando o método upload_file do SDK (RECOMENDADO)
  result = client.memories.upload_file(
      file=open('document.pdf', 'rb'),
      container_tags='research_project'  # Parâmetro do tipo string
  )

  # Método 2: Usando requests com form data
  import requests

  files = {'file': open('document.pdf', 'rb')}
  data = {'containerTags': 'research_project'}

  response = requests.post(
      'https://api.supermemory.ai/v3/documents/file',
      headers={'Authorization': f'Bearer {api_key}'},
      files=files,
      data=data
  )

  result = response.json()
  print(result)
  # Saída: {'id': 'Mx7fK9pL2qR5tE8yU4nC7', 'status': 'processing'}
  ```
</CodeGroup>

<div id="supported-file-types">
  #### Tipos de arquivo compatíveis
</div>

<Tabs>
  <Tab title="Documents">
    * **PDF**: Extração com suporte a OCR para documentos digitalizados
    * **Google Docs**: Via integração com a API do Google Drive
    * **Google Sheets**: Extração de conteúdo de planilhas
    * **Google Slides**: Extração de conteúdo de apresentações
    * **Notion Pages**: Conteúdo avançado com preservação da estrutura de blocos
    * **OneDrive Documents**: Documentos do Microsoft Office
  </Tab>

  <Tab title="Media">
    * **Imagens**: JPG, PNG, GIF, WebP com extração de texto via OCR
    * **Vídeos**: MP4, WebM, AVI com transcrição (YouTube, Vimeo)
  </Tab>

  <Tab title="Web Content">
    * **Páginas da Web**: Qualquer URL pública com extração inteligente de conteúdo
    * **Publicações do Twitter/X**: Conteúdo de tweets e metadata
    * **Vídeos do YouTube**: Transcrição automática e metadata
  </Tab>

  <Tab title="Text Formats">
    * **Texto simples**: Arquivos TXT, MD, CSV
  </Tab>
</Tabs>

<div id="content-types-processing">
  ## Tipos de conteúdo e processamento
</div>

<div id="automatic-detection">
  ### Detecção automática
</div>

A Supermemory detecta automaticamente tipos de conteúdo com base em:

* **Padrões de URL**: análise de domínio e caminho para serviços específicos
* **Tipos MIME**: detecção do tipo de arquivo a partir de cabeçalhos/metadata
* **Análise de conteúdo**: inspeção de estrutura e formato
* **Extensões de arquivo**: método de identificação de fallback

```typescript

type MemoryType =
  | 'text'        // Conteúdo de texto simples
  | 'pdf'         // documentos em PDF
  | 'tweet'       // Publicações no Twitter/X
  | 'google_doc'  // Google Docs
  | 'google_slide'// Google Slides
  | 'google_sheet'// Google Sheets
  | 'image'       // Imagens com OCR (Reconhecimento Óptico de Caracteres)
  | 'video'       // Vídeos com transcrição
  | 'notion_doc'  // Páginas do Notion
  | 'webpage'     // Páginas da Web
  | 'onedrive'    // documentos do OneDrive



// Exemplos de detecção automática
const examples = {
  "https://twitter.com/user/status/123": "tweet",
  "https://youtube.com/watch?v=abc": "video",
  "https://docs.google.com/document/d/123": "google_doc",
  "https://docs.google.com/spreadsheets/d/123": "google_sheet",
  "https://docs.google.com/presentation/d/123": "google_slide",
  "https://notion.so/page-123": "notion_doc",
  "https://example.com": "webpage",
  "Regular text content": "text",
  // Arquivos PDF enviados → "pdf"
  // Arquivos de imagem enviados → "image"
  // Links do OneDrive → "onedrive"
}
```

<div id="processing-pipeline">
  ### Pipeline de Processamento
</div>

Cada tipo de conteúdo segue um pipeline de processamento especializado:

<Accordion title="Conteúdo de Texto" defaultOpen>
  O conteúdo é limpo, normalizado e segmentado em chunks para recuperação ideal:

  1. **Em fila**: A memory entra na fila de processamento
  2. **Extraindo**: Normalização e limpeza do texto
  3. **Segmentação em chunks**: Divisão inteligente com base na estrutura do conteúdo
  4. **Embedding**: Conversão em representações vetoriais para busca
  5. **Indexação**: Adição ao índice pesquisável
  6. **Concluído:** Extração de metadata concluída
</Accordion>

<Accordion title="Conteúdo da Web">
  Páginas da web passam por uma extração de conteúdo sofisticada:

  1. **Em fila:** url em fila para processamento
  2. **Extraindo**: Obter o conteúdo da página com cabeçalhos adequados, remover navegação e boilerplate, extrair title, descrição etc.
  3. **Segmentação em chunks:** Conteúdo dividido para recuperação ideal
  4. **Embedding**: Geração de representação vetorial
  5. **Indexação**: Adição ao índice de busca
  6. **Concluído:** Processamento concluído com `type: 'webpage'`
</Accordion>

<Accordion title="Processamento de Arquivos">
  Arquivos são processados por extratores especializados:

  1. **Em fila**: Arquivo em fila para processamento
  2. **Extração de Conteúdo**: Detecção de tipo e processamento específico do formato
  3. **OCR/Transcrição**: Para imagens e arquivos de mídia
  4. **Segmentação em chunks:** Conteúdo dividido em segmentos pesquisáveis
  5. **Embedding:** Criação de representação vetorial
  6. **Indexação:** Adição ao índice de busca
  7. **Concluído:** Processamento concluído
</Accordion>

<div id="error-handling">
  ## Tratamento de erros
</div>

<div id="common-errors">
  ### Erros Comuns
</div>

Role para a direita para ver mais.

<Tabs>
  <Tab title="Erros de Autenticação">
    ```json
    // AuthenticationError class
    {
      name: "AuthenticationError",
      status: 401,
      message: "401 Unauthorized",
      error: {
        message: "Invalid API key",
        type: "authentication_error"
      }
    }
    ```

    **Causas:**

    * Chave da API ausente ou inválida
    * Token de autenticação expirado
    * Formato incorreto do cabeçalho de autorização
  </Tab>

  <Tab title="Erros de Requisição Inválida (400)">
    ```json
    // BadRequestError class
    {
      name: "BadRequestError",
      status: 400,
      message: "400 Bad Request",
      error: {
        message: "Invalid request parameters",
        details: {
          content: "Content cannot be empty",
          customId: "customId exceeds maximum length"
        }
      }
    }
    ```

    **Causas:**

    * Campos obrigatórios ausentes
    * Tipos de parâmetros inválidos
    * Conteúdo muito grande
    * customId muito longo
    * Estrutura de metadata inválida
  </Tab>

  <Tab title="Limite de Taxa (429)">
    ```json
    // RateLimitError class
    {
      name: "RateLimitError",
      status: 429,  // NOT 402!
      message: "429 Too Many Requests",
      error: {
        message: "Rate limit exceeded",
        retry_after: 60
      }
    }
    ```

    **Causas:**

    * Cota mensal de tokens excedida
    * Limites de taxa excedidos
    * Limites da assinatura atingidos

    **Correção:** Implemente backoff exponencial e respeite os limites de taxa
  </Tab>

  <Tab title="Erros de Não Encontrado (404)">
    ```json
    // NotFoundError class
    {
      name: "NotFoundError",
      status: 404,
      message: "404 Not Found",
      error: {
        message: "Memory not found",
        resource_id: "invalid_memory_id"
      }
    }
    ```

    **Causas:**

    * O ID da memory não existe
    * A memory foi excluída
    * URL de endpoint inválida
  </Tab>

  <Tab title="Permissão Negada (403)">
    ```json
    // PermissionDeniedError class
    {
      name: "PermissionDeniedError",
      status: 403,
      message: "403 Forbidden",
      error: {
        message: "Insufficient permissions",
        required_permission: "memories:write"
      }
    }
    ```

    **Causas:**

    * A chave da API não possui as permissões necessárias
    * Acesso a recursos restritos
    * Limitações da conta
  </Tab>

  <Tab title="Erros de Servidor (500+)">
    ```json
    // InternalServerError class
    {
      name: "InternalServerError",
      status: 500,
      message: "500 Internal Server Error",
      error: {
        message: "Processing failed",
        details: "Content extraction service unavailable"
      }
    }
    ```

    **Causas:**

    * Serviço externo indisponível
    * Falha na extração de conteúdo
  </Tab>

  <Tab title="Erros de Rede">
    ```json
        // APIConnectionError class - NEW
      {
        name: "APIConnectionError",
        message: "Connection error.",
        cause: Error // Original network error
      }

      // APIConnectionTimeoutError class - NEW
      {
        name: "APIConnectionTimeoutError",
        message: "Request timed out."
      }
    ```

    **Causas:**

    * Problemas de conectividade de rede
    * Falhas na resolução de DNS
    * Timeouts de requisição
    * Bloqueio por proxy/firewall
  </Tab>
</Tabs>

<div id="best-practices">
  ## Boas práticas
</div>

<div id="container-tags-optimize-for-performance">
  ### Tags de contêiner: otimize o desempenho
</div>

Use uma única tag de contêiner para melhor desempenho de consulta. Várias tags são suportadas, mas aumentam a Latency.

```json
{
  "content": "Fluxo de autenticação atualizado para usar tokens JWT",
  "containerTags": "[project_alpha]",
  "metadata": {
    "type": "alteração_técnica",
    "author": "sarah_dev",
    "impact": "breaking"
  }
}
```

**Tag única vs múltiplas tags**

```javascript
// ✅ Recomendado: Tag única, consultas mais rápidas
{ "containerTags": ["project_alpha"] }

// ⚠️ Permitido, porém mais lento: Várias tags aumentam a Latency
{ "containerTags": ["project_alpha", "auth", "backend"] }
```

**Por que tags únicas têm melhor desempenho:**

* Memórias no mesmo espaço podem fazer referência umas às outras com eficiência
* Consultas de pesquisa não precisam percorrer vários espaços
* A inferência de integração é mais rápida dentro de um único espaço

<div id="custom-ids-deduplication-and-updates">
  ### IDs personalizados: desduplicação e atualizações
</div>

IDs personalizados evitam duplicatas e permitem atualizações de documentos. Dois métodos de atualização disponíveis.

**Método 1: POST com customId (Upsert)**

```bash
# Criar documento
curl -X POST "https://api.supermemory.ai/v3/documents" \
  -H "Authorization: Bearer $SUPERMEMORY_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "content": "A API usa endpoints REST",
    "customId": "api_docs_v1",
    "containerTags": ["project_alpha"]
  }'
# Resposta: {"id": "abc123", "status": "queued"}

# Atualizar o mesmo documento (mesmo customId = upsert)
curl -X POST "https://api.supermemory.ai/v3/documents" \
  -H "Authorization: Bearer $SUPERMEMORY_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "content": "A API migrou para GraphQL",
    "customId": "api_docs_v1",
    "containerTags": ["project_alpha"]
  }'
```

**Método 2: PATCH por id (atualização)**

```bash
curl -X PATCH "https://api.supermemory.ai/v3/documents/abc123" \
  -H "Authorization: Bearer $SUPERMEMORY_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "content": "A API agora usa GraphQL com cache",
    "metadata": {"version": 3}
  }'
```

**Padrões personalizados de ID**

```javascript
// Sincronização com sistemas externos
"jira_PROJ_123"
"confluence_456789"
"github_issue_987"

// Entidades do banco de dados
"user_profile_12345"
"order_67890"

// Conteúdo versionado
"meeting_2024_01_15"
"api_docs_auth"
"requirements_v3"
```

**Comportamento da atualização**

* Memórias antigas são excluídas
* Novas memórias são criadas a partir do conteúdo atualizado
* O mesmo ID do documento é mantido

### Limites de taxa &amp; cotas

**Uso de tokens**

```javascript
"Olá, mundo" // ≈ 2 tokens
"PDF de 10 páginas" // ≈ 2.000–4.000 tokens
"Vídeo do YouTube (10 min)" // ≈ 1.500–3.000 tokens
"Artigo da web" // ≈ 500–2.000 tokens
```

**Limites atuais**

| Recurso | Free | Starter | Growth |
|---------|------|-----|------------|
| Tokens de memory por mês | 100.000 | 1.000.000 | 10.000.000 |
| Consultas de busca por mês | 1.000 | 10.000 | 100.000 |

**Resposta quando o limite é excedido**

```bash
curl -X POST "https://api.supermemory.ai/v3/documents" \
  -H "Authorization: Bearer your_api_key" \
  -d '{"content": "Algum conteúdo"}'
```

Resposta:

```json
{"error": "Limite de tokens da memory atingido", "status": 402}
```

<div id="batch-upload-of-documents">
  ## Upload em lote de documents
</div>

Processe grandes volumes com eficiência usando limitação de taxa e recuperação de erros.

<div id="implementation-strategy">
  ### Estratégia de implementação
</div>

<Tabs>
  <Tab title="TypeScript">
    ```typescript
    import Supermemory, {
      BadRequestError,
      RateLimitError,
      AuthenticationError
    } from 'supermemory';

    interface Document {
      id: string;
      content: string;
      title?: string;
      createdAt?: string;
      metadata?: Record<string, string | number | boolean>;
    }

    async function batchIngest(documents: Document[], options = {}) {
      const {
        batchSize = 5,
        delayBetweenBatches = 2000,
        maxRetries = 3
      } = options;

      const results = [];

      for (let i = 0; i < documents.length; i += batchSize) {
        const batch = documents.slice(i, i + batchSize);
        console.log(`Processando lote ${Math.floor(i/batchSize) + 1}/${Math.ceil(documents.length/batchSize)}`);

        const batchResults = await Promise.allSettled(
          batch.map(doc => ingestWithRetry(doc, maxRetries))
        );

        results.push(...batchResults);

        // Limitação de taxa entre lotes
        if (i + batchSize < documents.length) {
          await new Promise(resolve => setTimeout(resolve, delayBetweenBatches));
        }
      }

      return results;
    }

    async function ingestWithRetry(doc: Document, maxRetries: number) {
      for (let attempt = 1; attempt <= maxRetries; attempt++) {
        try {
          return await client.memories.add({
            content: doc.content,
            customId: doc.id,
            containerTags: ["batch_import_user_123"], // CORRIGIDO: array
            metadata: {
              source: "migration",
              batch_id: generateBatchId(),
              original_created: doc.createdAt || new Date().toISOString(),
              title: doc.title || "",
              ...doc.metadata
            }
          });
        } catch (error) {
          // CORRIGIDO: tratamento adequado de erros
          if (error instanceof AuthenticationError) {
            console.error('Falha na autenticação — verifique a chave da API');
            throw error; // Don't retry auth errors
            throw error; // Não repetir em erros de autenticação

          if (error instanceof BadRequestError) {
            console.error('Formato de documento inválido:', doc.id);
            throw error; // Não repetir em erros de validação
          }

          if (error instanceof RateLimitError) {
            console.log(`Limite de taxa atingido na tentativa ${attempt}, aguardando mais tempo...`);
            const delay = Math.pow(2, attempt) * 2000; // Atrasos maiores para limites de taxa
            await new Promise(resolve => setTimeout(resolve, delay));
            continue;
          }

          if (attempt === maxRetries) throw error;

          // Backoff exponencial para outros erros
          const delay = Math.pow(2, attempt) * 1000;
          console.log(`Nova tentativa ${attempt}/${maxRetries} para ${doc.id} em ${delay} ms`);
          await new Promise(resolve => setTimeout(resolve, delay));
        }
      }
    }

    function generateBatchId(): string {
      return `batch_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
    }
    ```
  </Tab>

  <Tab title="Python">
    ```python
        import asyncio
    import time
    import logging
    from typing import List, Dict, Any, Optional
    from supermemory import Supermemory, BadRequestError, RateLimitError

    async def batch_ingest(
        documents: List[Dict[str, Any]],
        options: Optional[Dict[str, Any]] = None
    ):
        options = options or {}
        batch_size = options.get('batch_size', 5)  # CORRIGIDO: tamanho conservador
        delay_between_batches = options.get('delay_between_batches', 2.0)  # CORRIGIDO: 2 segundos
        max_retries = options.get('max_retries', 3)

        results = []

        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(documents) + batch_size - 1) // batch_size

            print(f"Processando lote {batch_num}/{total_batches}")

            # Processar lote com tratamento adequado de erros
            tasks = [ingest_with_retry(doc, max_retries) for doc in batch]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)

            results.extend(batch_results)

            # Aplicar limitação de taxa entre os lotes
            if i + batch_size < len(documents):
                await asyncio.sleep(delay_between_batches)

        return results

    async def ingest_with_retry(doc: Dict[str, Any], max_retries: int):
        for attempt in range(1, max_retries + 1):
            try:
                return await client.memories.add(
                    content=doc['content'],
                    custom_id=doc['id'],
                    container_tags=["batch_import_user_123"],  # CORRIGIDO: lista
                    metadata={
                        "source": "migration",
                        "batch_id": generate_batch_id(),
                        "original_created": doc.get('created_at', ''),
                        "title": doc.get('title', ''),
                        **doc.get('metadata', {})
                    }
                )
            except BadRequestError as e:
                logging.error(f"Documento inválido {doc['id']}: {e}")
                raise  # Não tentar novamente erros de validação

            except RateLimitError as e:
                logging.warning(f"Limitação de taxa na tentativa {attempt}")
                delay = 2 ** attempt * 2  # Atrasos maiores para limites de taxa
                await asyncio.sleep(delay)
                continue

            except Exception as error:
                if attempt == max_retries:
                    raise error

                # Retrocesso exponencial
                delay = 2 ** attempt
                logging.info(f"Nova tentativa {attempt}/{max_retries} para {doc['id']} em {delay}s")
                await asyncio.sleep(delay)

    def generate_batch_id() -> str:
        import random
        import string
        return f"batch_{int(time.time())}_{random.choices(string.ascii_lowercase, k=8)}"
    ```
  </Tab>
</Tabs>

<div id="best-practices-for-batch-operations">
  ### Melhores práticas para operações em lote
</div>

<Accordion title="Otimização de desempenho" defaultOpen>
  * **Tamanho do lote**: 3-5 documents por vez
  * **Atrasos**: 2-3 segundos entre lotes evitam rate limiting
  * **Promise.allSettled()**: Lida com resultados mistos de sucesso/falha
  * **Acompanhamento do progresso**: Monitore operações de longa duração

  **Exemplo de saída**

  ```
  Processando lote 1/50 (documents 1-3)
  Processados com sucesso: 2/3 documents
  Falhas: 1/3 documents (BadRequestError: Invalid content)
  Progresso: 3/150 (2,0%) - Próximo lote em 2s
  ```
</Accordion>

<Accordion title="Tratamento de erros">
  * **Tipos de erro específicos:** Trate `BadRequestError`, `RateLimitError`, `AuthenticationError` de forma diferente
  * **Sem lógica de nova tentativa**: Não reexecute erros de validação ou autenticação
  * **Tratamento de limite de taxa**: Aumente o backoff para erros de limite de taxa
  * **Registro**: Registre falhas para revisão/tentativa posterior
</Accordion>

<Accordion title="Gerenciamento de memória">
  * **Streaming**: Processe arquivos grandes em chunks
  * **Limpeza**: Limpe da memória os lotes já processados
  * **Persistência de progresso**: Retome migrações interrompidas
</Accordion>

<Note>
  Pronto para começar a ingestão? [Obtenha uma chave da API](https://console.supermemory.ai) agora!
</Note>