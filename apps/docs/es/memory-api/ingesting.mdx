---
title: "Ingesta de documentos y datos"
sidebarTitle: "Guía de ingesta de contenido"
description: "Guía completa para la ingesta de texto, URL, archivos y diversos tipos de contenido en Supermemory"
---

Supermemory ofrece un sistema de ingesta potente y flexible que puede procesar prácticamente cualquier tipo de contenido. Ya sea que agregues notas de texto simples, páginas web, PDF, imágenes o documentos complejos de distintas plataformas, nuestra API lo gestiona todo sin inconvenientes.

<div id="understanding-the-mental-model">
  ## Comprender el modelo mental
</div>

Antes de profundizar en la API, es importante entender cómo Supermemory procesa tu contenido:

<div id="documents-vs-memories">
  ### Documentos vs memories
</div>

* **Documentos**: Cualquier cosa que agregues a Supermemory (archivos, URLs, texto) se considera un **documento**
* **Memories**: Los documentos se dividen automáticamente en partes más pequeñas y buscables llamadas **memories**

Cuando usas el endpoint &quot;Add Memory&quot;, en realidad estás agregando un **documento**. La tarea de Supermemory es dividir inteligentemente ese documento en **memories** óptimas que se puedan buscar y recuperar.

```
Tu contenido → Documento → Procesamiento → Varias memories
     ↓             ↓           ↓            ↓
   Archivo PDF → Documento almacenado → Segmentación en chunks → memories consultables
```

Puedes visualizar este proceso en la [Supermemory Console](https://console.supermemory.ai), donde verás una vista de grafo que muestra cómo tus Documentos se descomponen en memories interconectadas.

<div id="content-sources">
  ### Fuentes de contenido
</div>

Supermemory admite contenido mediante tres métodos principales:

1. **API directa**: Carga archivos o envía contenido a través de endpoints de la API
2. **Conectores**: Integraciones automatizadas con plataformas como Google Drive, Notion y OneDrive ([más información sobre conectores](/es/connectors))
3. **Procesamiento de URL**: Extracción automática de contenido de páginas web, videos y redes sociales

<div id="overview">
  ## Descripción general
</div>

El sistema de ingesta consta de varios componentes clave:

* **Múltiples métodos de entrada**: contenido JSON, carga de archivos y procesamiento de URL
* **Procesamiento asíncrono**: flujos de trabajo en segundo plano gestionan la extracción de contenido y la segmentación en chunks
* **Detección automática de contenido**: identifica y procesa automáticamente distintos tipos de contenido
* **Organización por espacios**: etiquetas de contenedor agrupan memories relacionadas para mejorar la inferencia de contexto
* **Seguimiento del estado**: actualizaciones de status en tiempo real a lo largo del pipeline de procesamiento

<div id="how-it-works">
  ### Cómo funciona
</div>

<Steps>
  <Step title="Submit Document">
    Envía tu contenido (texto, archivo o URL) para crear un nuevo documento
  </Step>

  <Step title="Validation">
    La API valida la solicitud y comprueba los límites de velocidad y cuotas
  </Step>

  <Step title="Document Storage">
    Tu contenido se almacena como documento y se pone en cola para su procesamiento
  </Step>

  <Step title="Content Extraction">
    Extractores especializados procesan el documento según su tipo
  </Step>

  <Step title="Memory Creation">
    El documento se divide inteligentemente en múltiples memories que se pueden buscar
  </Step>

  <Step title="Embedding & Indexing">
    Las memories se convierten en embeddings vectoriales y se indexan para que se puedan buscar
  </Step>
</Steps>

<div id="ingestion-endpoints">
  ## Endpoints de ingesta
</div>

<div id="add-document-json-content">
  ### Agregar documento: contenido JSON
</div>

El endpoint principal para añadir contenido que se procesará en documentos.

**Endpoint:** `POST /v3/documents`

<Note>
  A pesar del nombre del endpoint, estás creando un **documento** que Supermemory segmentará automáticamente en **memories** buscables.
</Note>

<CodeGroup>
  ```bash cURL
  curl https://api.supermemory.ai/v3/documents \
    -H "Authorization: Bearer $SUPERMEMORY_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
      "content": "Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without explicit programming.",
      "containerTags": ["ai-research", "user_123"],
      "metadata": {
        "source": "research-notes",
        "category": "education",
        "priority": "high"
      },
      "customId": "ml-basics-001"
    }'
  ```

  ```typescript TypeScript
  import Supermemory from 'supermemory'

  const client = new Supermemory({
    apiKey: process.env.SUPERMEMORY_API_KEY
  })

  async function addContent() {
      const result = await client.memories.add({
          content: "Machine learning is a subset of artificial intelligence...",
          containerTags: ["ai-research"],
          metadata: {
            source: "research-notes",
            category: "education",
            priority: "high"
          },
          customId: "ml-basics-001"
        })

        console.log(result) // { id: "abc123", status: "queued" }
  }

   addContent()
  ```

  ```python Python
  from supermemory import Supermemory
  import os

  client = Supermemory(api_key=os.environ.get("SUPERMEMORY_API_KEY"))

  result = client.memories.add(
      content="Machine learning is a subset of artificial intelligence...",
      container_tags=["ai-research"],
      metadata={
          "source": "research-notes",
          "category": "education",
          "priority": "high"
      },
      custom_id="ml-basics-001"
  )

  print(result)  # { "id": "abc123", "status": "queued" }
  ```
</CodeGroup>

<div id="request-parameters">
  #### Parámetros de la solicitud
</div>

| Parámetro | Tipo | Requerido | Descripción |
|-----------|------|-----------|-------------|
| `content` | string | Sí | El contenido que se procesará en un documento. Puede ser texto, URL u otros formatos compatibles |
| `containerTag` | string | No | **Recomendado**: Etiqueta única para agrupar memories relacionadas en un espacio. Valor predeterminado: &quot;sm&#95;project&#95;default&quot; |
| `containerTags` | string[] | No | Formato de matriz heredado. Usa `containerTag` en su lugar para un mejor rendimiento |
| `metadata` | object | No | metadata adicional de pares clave-valor (solo strings, números y booleanos) |
| `customId` | string | No | Tu propio identificador para este documento (máx. 255 caracteres) |
| `raw` | string | No | Contenido en bruto para almacenar junto con el contenido procesado |

<div id="response">
  #### Respuesta
</div>

Cuando crees un documento correctamente, recibirás una confirmación sencilla con el id del documento y su status de procesamiento inicial:

```json
{
  "id": "D2Ar7Vo7ub83w3PRPZcaP1",
  "status": "queued"
}
```

**Qué significa esto:**

* `id`: El identificador único de tu documento; guárdalo para hacer seguimiento del procesamiento o para consultarlo después
* `status`: Estado actual de procesamiento. `"queued"` significa que está esperando a ser procesado en memories

<Note>
  El documento comienza a procesarse de inmediato en segundo plano. En cuestión de segundos o minutos (según el tamaño del contenido), se dividirá en memories buscables.
</Note>

<div id="file-upload-drop-and-process">
  ### Carga de archivos: suelta y procesa
</div>

¿Tienes un PDF, una imagen o un video? Súbelo directamente y deja que Supermemory extraiga automáticamente el contenido valioso.

**Endpoint:** `POST /v3/documents/file`

**Por qué esto es potente:** En lugar de copiar texto manualmente de PDFs o transcribir videos, simplemente sube el archivo. Supermemory se encarga del OCR para imágenes, la transcripción para videos y la extracción inteligente de texto para documentos.

<CodeGroup>
  ```bash cURL
  curl https://api.supermemory.ai/v3/documents/file \
    -H "Authorization: Bearer $SUPERMEMORY_API_KEY" \
    -F "file=@document.pdf" \
    -F "containerTags=research_project"

  # Response:
  # {
  #   "id": "Mx7fK9pL2qR5tE8yU4nC7",
  #   "status": "processing"
  # }
  ```

  ```typescript TypeScript
  import Supermemory from 'supermemory'
  import fs from 'fs'

  const client = new Supermemory({
    apiKey: process.env.SUPERMEMORY_API_KEY
  })

  // Method 1: Using SDK uploadFile method (RECOMMENDED)
  const result = await client.memories.uploadFile({
    file: fs.createReadStream('/path/to/document.pdf'),
    containerTags: 'research_project'  // String, not array!
  })

  // Method 2: Using fetch with form data (for browser/manual implementation)
  const formData = new FormData()
  formData.append('file', fileInput.files[0])
  formData.append('containerTags', 'research_project')

  const response = await fetch('https://api.supermemory.ai/v3/documents/file', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.SUPERMEMORY_API_KEY}`
    },
    body: formData
  })

  const result = await response.json()
  console.log(result)
  // Output: { id: "Mx7fK9pL2qR5tE8yU4nC7", status: "processing" }
  ```

  ```python Python
  from supermemory import Supermemory

  client = Supermemory(api_key="your_api_key")

  # Method 1: Using SDK upload_file method (RECOMMENDED)
  result = client.memories.upload_file(
      file=open('document.pdf', 'rb'),
      container_tags='research_project'  # String parameter name
  )

  # Method 2: Using requests with form data
  import requests

  files = {'file': open('document.pdf', 'rb')}
  data = {'containerTags': 'research_project'}

  response = requests.post(
      'https://api.supermemory.ai/v3/documents/file',
      headers={'Authorization': f'Bearer {api_key}'},
      files=files,
      data=data
  )

  result = response.json()
  print(result)
  # Output: {'id': 'Mx7fK9pL2qR5tE8yU4nC7', 'status': 'processing'}
  ```
</CodeGroup>

<div id="supported-file-types">
  #### Tipos de archivos compatibles
</div>

<Tabs>
  <Tab title="Documents">
    * **PDF**: Extraídos con soporte de OCR (Reconocimiento Óptico de Caracteres) para documentos escaneados
    * **Google Docs**: Mediante integración con la API de Google Drive
    * **Google Sheets**: Extracción de contenido de hojas de cálculo
    * **Google Slides**: Extracción de contenido de presentaciones
    * **Páginas de Notion**: Contenido enriquecido con preservación de la estructura de bloques
    * **Documentos de OneDrive**: Documentos de Microsoft Office
  </Tab>

  <Tab title="Media">
    * **Imágenes**: JPG, PNG, GIF, WebP con extracción de texto mediante OCR
    * **Vídeos**: MP4, WebM, AVI con transcripción (YouTube, Vimeo)
  </Tab>

  <Tab title="Web Content">
    * **Páginas web**: Cualquier url pública con extracción inteligente de contenido
    * **Publicaciones de Twitter/X**: Contenido de tweets y metadata
    * **Vídeos de YouTube**: Transcripción automática y metadata
  </Tab>

  <Tab title="Text Formats">
    * **Texto sin formato**: Archivos TXT, MD, CSV
  </Tab>
</Tabs>

<div id="content-types-processing">
  ## Tipos de contenido y procesamiento
</div>

<div id="automatic-detection">
  ### Detección automática
</div>

Supermemory detecta automáticamente los tipos de contenido en función de:

* **Patrones de URL**: análisis de dominio y ruta para servicios específicos
* **Tipos MIME**: detección del tipo de archivo a partir de encabezados/metadata
* **Análisis de contenido**: inspección de la estructura y el formato
* **Extensiones de archivo**: método de identificación de respaldo

```typescript

type MemoryType =
  | 'text'        // Contenido de texto sin formato
  | 'pdf'         // Documentos PDF
  | 'tweet'       // Publicaciones de Twitter/X
  | 'google_doc'  // Google Docs
  | 'google_slide'// Google Slides
  | 'google_sheet'// Google Sheets
  | 'image'       // Imágenes con OCR (Reconocimiento Óptico de Caracteres)
  | 'video'       // Videos con transcripción
  | 'notion_doc'  // Páginas de Notion
  | 'webpage'     // Páginas web
  | 'onedrive'    // Documentos de OneDrive



// Ejemplos de detección automática
const examples = {
  "https://twitter.com/user/status/123": "tweet",
  "https://youtube.com/watch?v=abc": "video",
  "https://docs.google.com/document/d/123": "google_doc",
  "https://docs.google.com/spreadsheets/d/123": "google_sheet",
  "https://docs.google.com/presentation/d/123": "google_slide",
  "https://notion.so/page-123": "notion_doc",
  "https://example.com": "webpage",
  "Regular text content": "text",
  // Archivos PDF subidos → "pdf"
  // Archivos de imagen subidos → "image"
  // Enlaces de OneDrive → "onedrive"
}
```

<div id="processing-pipeline">
  ### Flujo de procesamiento
</div>

Cada tipo de contenido sigue un flujo de procesamiento especializado:

<Accordion title="Text Content" defaultOpen>
  El contenido se limpia, se normaliza y se segmenta en chunks para una recuperación óptima:

  1. **En cola**: La memory entra en la cola de procesamiento
  2. **Extrayendo**: Normalización y limpieza del texto
  3. **Segmentación en chunks**: División inteligente basada en la estructura del contenido
  4. **Embedding**: Conversión a representaciones vectoriales para búsqueda
  5. **Indexación**: Incorporación al índice de búsqueda
  6. **Listo:** Extracción de metadata completada
</Accordion>

<Accordion title="Web Content">
  Las páginas web pasan por una extracción de contenido avanzada:

  1. **En cola:** url en cola para procesamiento
  2. **Extrayendo**: Obtener el contenido de la página con encabezados adecuados, eliminar navegación y boilerplate, extraer title, descripción, etc.
  3. **Segmentación en chunks:** División del contenido para una recuperación óptima
  4. **Embedding**: Generación de representación vectorial
  5. **Indexación**: Incorporación al índice de búsqueda
  6. **Listo:** Procesamiento completo con `type: 'webpage'`
</Accordion>

<Accordion title="File Processing">
  Los archivos se procesan mediante extractores especializados:

  1. **En cola**: Archivo en cola para procesamiento
  2. **Extracción de contenido**: Detección de tipo y procesamiento específico del formato
  3. **OCR/Transcription**: Para imágenes y archivos multimedia
  4. **Segmentación en chunks:** Desglose del contenido en segmentos buscables
  5. **Embedding:** Creación de representación vectorial
  6. **Indexación:** Incorporación al índice de búsqueda
  7. **Listo:** Procesamiento completado
</Accordion>

<div id="error-handling">
  ## Gestión de errores
</div>

<div id="common-errors">
  ### Errores comunes
</div>

Desplázate a la derecha para ver más.

<Tabs>
  <Tab title="Errores de autenticación">
    ```json
    // AuthenticationError class
    {
      name: "AuthenticationError",
      status: 401,
      message: "401 Unauthorized",
      error: {
        message: "Invalid API key",
        type: "authentication_error"
      }
    }
    ```

    **Causas:**

    * Falta la clave de API o es inválida
    * Token de autenticación expirado
    * Formato incorrecto del encabezado de autorización
  </Tab>

  <Tab title="Errores de solicitud incorrecta (400)">
    ```json
    // BadRequestError class
    {
      name: "BadRequestError",
      status: 400,
      message: "400 Bad Request",
      error: {
        message: "Invalid request parameters",
        details: {
          content: "Content cannot be empty",
          customId: "customId exceeds maximum length"
        }
      }
    }
    ```

    **Causas:**

    * Campos obligatorios faltantes
    * Tipos de parámetros inválidos
    * Contenido demasiado grande
    * ID personalizado demasiado largo
    * Estructura de metadata inválida
  </Tab>

  <Tab title="Limitación de tasa (429)">
    ```json
    // RateLimitError class
    {
      name: "RateLimitError",
      status: 429,  // NOT 402!
      message: "429 Too Many Requests",
      error: {
        message: "Rate limit exceeded",
        retry_after: 60
      }
    }
    ```

    **Causas:**

    * Cuota mensual de tokens excedida
    * Límites de tasa excedidos
    * Límites de suscripción alcanzados

    **Solución:** Implementa backoff exponencial y respeta los límites de tasa
  </Tab>

  <Tab title="Errores de no encontrado (404)">
    ```json
    // NotFoundError class
    {
      name: "NotFoundError",
      status: 404,
      message: "404 Not Found",
      error: {
        message: "Memory not found",
        resource_id: "invalid_memory_id"
      }
    }
    ```

    Causas:

    * El id de la memory no existe
    * La memory fue eliminada
    * URL de endpoint inválida
  </Tab>

  <Tab title="Permiso denegado (403)">
    ```json
    // PermissionDeniedError class
    {
      name: "PermissionDeniedError",
      status: 403,
      message: "403 Forbidden",
      error: {
        message: "Insufficient permissions",
        required_permission: "memories:write"
      }
    }
    ```

    Causas:

    * La clave de API no tiene los permisos requeridos
    * Acceso a recursos restringidos
    * Limitaciones de la cuenta
  </Tab>

  <Tab title="Errores del servidor (500+)">
    ```json
    // InternalServerError class
    {
      name: "InternalServerError",
      status: 500,
      message: "500 Internal Server Error",
      error: {
        message: "Processing failed",
        details: "Content extraction service unavailable"
      }
    }
    ```

    **Causas:**

    * Servicio externo no disponible
    * Error en la extracción de contenido
  </Tab>

  <Tab title="Errores de red">
    ```json
        // APIConnectionError class - NEW
      {
        name: "APIConnectionError",
        message: "Connection error.",
        cause: Error // Original network error
      }

      // APIConnectionTimeoutError class - NEW
      {
        name: "APIConnectionTimeoutError",
        message: "Request timed out."
      }
    ```

    Causas:

    * Problemas de conectividad de red
    * Fallos de resolución de DNS
    * Tiempos de espera agotados
    * Bloqueo por proxy/firewall
  </Tab>
</Tabs>

<div id="best-practices">
  ## Buenas prácticas
</div>

<div id="container-tags-optimize-for-performance">
  ### Etiquetas de contenedor: optimiza el rendimiento
</div>

Usa una única etiqueta de contenedor para mejorar el rendimiento de las consultas. Se admiten varias etiquetas, pero aumentan la latencia.

```json
{
  "content": "Se actualizó el flujo de autenticación para usar tokens JWT",
  "containerTags": "[project_alpha]",
  "metadata": {
    "type": "technical_change",
    "author": "sarah_dev",
    "impact": "cambio incompatible"
  }
}
```

**Etiqueta única vs. múltiples etiquetas**

```javascript
// ✅ Recomendado: una sola etiqueta, consultas más rápidas
{ "containerTags": ["project_alpha"] }

// ⚠️ Permitido pero más lento: varias etiquetas aumentan la latencia
{ "containerTags": ["project_alpha", "auth", "backend"] }
```

**Por qué las etiquetas individuales funcionan mejor:**

* Las memories en el mismo espacio pueden referenciarse entre sí de forma eficiente
* Las consultas de búsqueda no necesitan atravesar varios espacios
* La inferencia de integración es más rápida dentro de un único espacio

<div id="custom-ids-deduplication-and-updates">
  ### IDs personalizados: deduplicación y actualizaciones
</div>

Los IDs personalizados evitan duplicados y permiten actualizar documentos. Hay dos métodos de actualización disponibles.

**Método 1: POST con customId (upsert)**

```bash
# Crear documento
curl -X POST "https://api.supermemory.ai/v3/documents" \
  -H "Authorization: Bearer $SUPERMEMORY_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "content": "La API usa endpoints REST",
    "customId": "api_docs_v1",
    "containerTags": ["project_alpha"]
  }'
# Respuesta: {"id": "abc123", "status": "queued"}

# Actualizar el mismo documento (mismo customId = upsert)
curl -X POST "https://api.supermemory.ai/v3/documents" \
  -H "Authorization: Bearer $SUPERMEMORY_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "content": "La API migró a GraphQL",
    "customId": "api_docs_v1",
    "containerTags": ["project_alpha"]
  }'
```

**Método 2: PATCH por id (actualización)**

```bash
curl -X PATCH "https://api.supermemory.ai/v3/documents/abc123" \
  -H "Authorization: Bearer $SUPERMEMORY_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "content": "La API ahora utiliza GraphQL con caché",
    "metadata": {"version": 3}
  }'
```

**Patrones de id personalizados**

```javascript
// Sincronización con sistemas externos
"jira_PROJ_123"
"confluence_456789"
"github_issue_987"

// Entidades de base de datos
"user_profile_12345"
"order_67890"

// Contenido versionado
"meeting_2024_01_15"
"api_docs_auth"
"requirements_v3"
```

**Comportamiento de la actualización**

* Se eliminan las memories antiguas
* Se crean nuevas memories a partir del contenido actualizado
* Se mantiene el mismo id del documento

<div id="rate-limits-quotas">
  ### Límites de uso y cuotas
</div>

**Uso de tokens**

```javascript
"Hola, mundo" // ≈ 2 tokens
"PDF de 10 páginas" // ≈ 2,000-4,000 tokens
"Video de YouTube (10 min)" // ≈ 1,500-3,000 tokens
"Artículo web" // ≈ 500-2,000 tokens
```

**Límites actuales**

| Función | Gratis | Starter | Growth |
|---------|------|-----|------------|
| Tokens de memory al mes | 100,000 | 1,000,000 | 10,000,000 |
| Consultas de búsqueda al mes | 1,000 | 10,000 | 100,000 |

**Respuesta cuando se supera el límite**

```bash
curl -X POST "https://api.supermemory.ai/v3/documents" \
  -H "Authorization: Bearer your_api_key" \
  -d '{"content": "Algún contenido"}'
```

Respuesta:

```json
{"error": "Se alcanzó el límite de tokens de memory", "status": 402}
```

<div id="batch-upload-of-documents">
  ## Carga por lotes de Documentos
</div>

Procesa grandes volúmenes de forma eficiente con limitación de velocidad y recuperación ante errores.

<div id="implementation-strategy">
  ### Estrategia de implementación
</div>

<Tabs>
  <Tab title="TypeScript">
    ```typescript
    import Supermemory, {
      BadRequestError,
      RateLimitError,
      AuthenticationError
    } from 'supermemory';

    interface Document {
      id: string;
      content: string;
      title?: string;
      createdAt?: string;
      metadata?: Record<string, string | number | boolean>;
    }

    async function batchIngest(documents: Document[], options = {}) {
      const {
        batchSize = 5,
        delayBetweenBatches = 2000,
        maxRetries = 3
      } = options;

      const results = [];

      for (let i = 0; i < documents.length; i += batchSize) {
        const batch = documents.slice(i, i + batchSize);
        console.log(`Procesando lote ${Math.floor(i/batchSize) + 1}/${Math.ceil(documents.length/batchSize)}`);

        const batchResults = await Promise.allSettled(
          batch.map(doc => ingestWithRetry(doc, maxRetries))
        );

        results.push(...batchResults);

        // Limitación de velocidad entre lotes
        if (i + batchSize < documents.length) {
          await new Promise(resolve => setTimeout(resolve, delayBetweenBatches));
        }
      }

      return results;
    }

    async function ingestWithRetry(doc: Document, maxRetries: number) {
      for (let attempt = 1; attempt <= maxRetries; attempt++) {
        try {
          return await client.memories.add({
            content: doc.content,
            customId: doc.id,
            containerTags: ["batch_import_user_123"], // CORREGIDO: Array
            metadata: {
              source: "migration",
              batch_id: generateBatchId(),
              original_created: doc.createdAt || new Date().toISOString(),
              title: doc.title || "",
              ...doc.metadata
            }
          });
        } catch (error) {
          // CORREGIDO: Manejo adecuado de errores
          if (error instanceof AuthenticationError) {
            console.error('Error de autenticación: verifica la clave de API');
            throw error; // Don't retry auth errors
            throw error; // No volver a intentar errores de autenticación

          if (error instanceof BadRequestError) {
            console.error('Formato de documento no válido:', doc.id);
            throw error; // No volver a intentar errores de validación
          }

          if (error instanceof RateLimitError) {
            console.log(`Se alcanzó el límite de velocidad en el intento ${attempt}, esperando más tiempo...`);
            const delay = Math.pow(2, attempt) * 2000; // Esperas más largas por límites de velocidad
            await new Promise(resolve => setTimeout(resolve, delay));
            continue;
          }

          if (attempt === maxRetries) throw error;

          // Reintentos con backoff exponencial para otros errores
          const delay = Math.pow(2, attempt) * 1000;
          console.log(`Reintento ${attempt}/${maxRetries} para ${doc.id} en ${delay} ms`);
          await new Promise(resolve => setTimeout(resolve, delay));
        }
      }
    }

    function generateBatchId(): string {
      return `batch_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
    }
    ```
  </Tab>

  <Tab title="Python">
    ```python
        import asyncio
    import time
    import logging
    from typing import List, Dict, Any, Optional
    from supermemory import Supermemory, BadRequestError, RateLimitError

    async def batch_ingest(
        documents: List[Dict[str, Any]],
        options: Optional[Dict[str, Any]] = None
    ):
        options = options or {}
        batch_size = options.get('batch_size', 5)  # CORREGIDO: Tamaño conservador
        delay_between_batches = options.get('delay_between_batches', 2.0)  # CORREGIDO: 2 segundos
        max_retries = options.get('max_retries', 3)

        results = []

        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(documents) + batch_size - 1) // batch_size

            print(f"Procesando lote {batch_num}/{total_batches}")

            # Procesar el lote con un manejo de errores adecuado
            tasks = [ingest_with_retry(doc, max_retries) for doc in batch]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)

            results.extend(batch_results)

            # Limitación de tasa entre lotes
            if i + batch_size < len(documents):
                await asyncio.sleep(delay_between_batches)

        return results

    async def ingest_with_retry(doc: Dict[str, Any], max_retries: int):
        for attempt in range(1, max_retries + 1):
            try:
                return await client.memories.add(
                    content=doc['content'],
                    custom_id=doc['id'],
                    container_tags=["batch_import_user_123"],  # CORREGIDO: Lista
                    metadata={
                        "source": "migration",
                        "batch_id": generate_batch_id(),
                        "original_created": doc.get('created_at', ''),
                        "title": doc.get('title', ''),
                        **doc.get('metadata', {})
                    }
                )
            except BadRequestError as e:
                logging.error(f"Documento inválido {doc['id']}: {e}")
                raise  # No volver a intentar errores de validación

            except RateLimitError as e:
                logging.warning(f"Limitado por tasa en el intento {attempt}")
                delay = 2 ** attempt * 2  # Retrasos más largos por límites de tasa
                await asyncio.sleep(delay)
                continue

            except Exception as error:
                if attempt == max_retries:
                    raise error

                # Reintento exponencial
                delay = 2 ** attempt
                logging.info(f"Reintento {attempt}/{max_retries} para {doc['id']} en {delay}s")
                await asyncio.sleep(delay)

    def generate_batch_id() -> str:
        import random
        import string
        return f"batch_{int(time.time())}_{random.choices(string.ascii_lowercase, k=8)}"
    ```
  </Tab>
</Tabs>

<div id="best-practices-for-batch-operations">
  ### Mejores prácticas para operaciones por lotes
</div>

<Accordion title="Optimización del rendimiento" defaultOpen>
  * **Tamaño del lote**: 3-5 documentos a la vez
  * **Pausas**: 2-3 segundos entre lotes para evitar la limitación de tasa
  * **Promise.allSettled()**: Gestiona resultados mixtos de éxito y fallo
  * **Seguimiento del progreso**: Supervisa operaciones de larga duración

  **Salida de ejemplo**

  ```
  Processing batch 1/50 (documents 1-3)
  Successfully processed: 2/3 documents
  Failed: 1/3 documents (BadRequestError: Invalid content)
  Progress: 3/150 (2.0%) - Next batch in 2s
  ```
</Accordion>

<Accordion title="Manejo de errores">
  * **Tipos de error específicos:** Trata `BadRequestError`, `RateLimitError`, `AuthenticationError` de forma diferenciada
  * **Sin reintentos**: No reintentes errores de validación o autenticación
  * **Gestión de límites de tasa**: Aumenta el backoff para errores de límite de tasa
  * **Registro**: Registra los fallos para revisión y posible reintento
</Accordion>

<Accordion title="Gestión de memoria">
  * **Streaming**: Procesa archivos grandes en chunks
  * **Limpieza**: Libera de la memory los lotes procesados
  * **Persistencia del progreso**: Reanuda migraciones interrumpidas
</Accordion>

<Note>
  ¿Listo para empezar a ingerir? ¡[Obtén una clave de API](https://console.supermemory.ai) ahora!
</Note>