---
title: "Ingestion de documents et de données"
sidebarTitle: "Guide d’ingestion de contenu"
description: "Guide complet pour ingérer du texte, des URL, des fichiers et divers types de contenu dans Supermemory"
---

Supermemory propose un système d’ingestion puissant et flexible, capable de traiter pratiquement tout type de contenu. Que vous ajoutiez de simples notes textuelles, des pages web, des PDF, des images ou des documents complexes issus de différentes plateformes, notre API prend tout en charge de façon fluide.

<div id="understanding-the-mental-model">
  ## Comprendre le modèle mental
</div>

Avant d’entrer dans l’API, il est important de comprendre comment Supermemory traite votre contenu :

<div id="documents-vs-memories">
  ### Documents vs Memories
</div>

* **Documents**: Tout ce que vous importez dans Supermemory (fichiers, URL, texte) est considéré comme un **document**
* **Memories**: Les documents sont automatiquement découpés en éléments plus petits, indexables et recherchables appelés **memories**

Lorsque vous utilisez l’endpoint « Add Memory », vous ajoutez en réalité un **document**. Le rôle de Supermemory est de fractionner intelligemment ce document en **memories** optimales qui peuvent être recherchées et retrouvées.

```
Votre contenu → Document → Traitement → Plusieurs memories
     ↓             ↓           ↓            ↓
   Fichier PDF → Document stocké → Segmentation en chunks → memories recherchables
```

Vous pouvez visualiser ce processus dans la [Supermemory Console](https://console.supermemory.ai), où vous verrez une vue graphique montrant comment vos documents sont décomposés en memories interconnectées.

<div id="content-sources">
  ### Sources de contenu
</div>

Supermemory accepte du contenu de trois manières principales :

1. **API directe** : Importez des fichiers ou envoyez du contenu via des endpoints d’API
2. **Connecteurs** : Intégrations automatisées avec des plateformes comme Google Drive, Notion et OneDrive ([en savoir plus sur les connecteurs](/fr/connectors))
3. **Traitement d’URL** : Extraction automatique à partir de pages web, de vidéos et des réseaux sociaux

<div id="overview">
  ## Aperçu
</div>

Le système d’ingestion se compose de plusieurs composants clés :

* **Méthodes d’entrée multiples** : contenu JSON, téléversement de fichiers et traitement d’URL
* **Traitement asynchrone** : des workflows en arrière-plan gèrent l’extraction de contenu et la segmentation en chunks
* **Détection automatique du contenu** : identifie et traite automatiquement différents types de contenu
* **Organisation par espaces** : des balises de conteneur regroupent les memories liées pour une meilleure inférence de contexte
* **Suivi du status** : mises à jour du status en temps réel tout au long du pipeline de traitement

<div id="how-it-works">
  ### Fonctionnement
</div>

<Steps>
  <Step title="Submit Document">
    Envoyez votre contenu (texte, fichier ou URL) pour créer un nouveau document
  </Step>

  <Step title="Validation">
    L’API valide la requête et vérifie les limites de débit/quotas
  </Step>

  <Step title="Document Storage">
    Votre contenu est stocké sous forme de document et mis en attente de traitement
  </Step>

  <Step title="Content Extraction">
    Des extracteurs spécialisés traitent le document en fonction de son type
  </Step>

  <Step title="Memory Creation">
    Le document est intelligemment découpé en plusieurs memories interrogeables
  </Step>

  <Step title="Embedding & Indexing">
    Les memories sont converties en embeddings vectoriels et rendues interrogeables
  </Step>
</Steps>

<div id="ingestion-endpoints">
  ## Points de terminaison d’ingestion
</div>

<div id="add-document-json-content">
  ### Ajouter un document - Contenu JSON
</div>

Point de terminaison principal pour ajouter du contenu qui sera traité en documents.

**Point de terminaison :** `POST /v3/documents`

<Note>
  Malgré le nom du point de terminaison, vous créez un **document** que Supermemory segmentera automatiquement en **memories** interrogeables.
</Note>

<CodeGroup>
  ```bash cURL
  curl https://api.supermemory.ai/v3/documents \
    -H "Authorization: Bearer $SUPERMEMORY_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
      "content": "Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without explicit programming.",
      "containerTags": ["ai-research", "user_123"],
      "metadata": {
        "source": "research-notes",
        "category": "education",
        "priority": "high"
      },
      "customId": "ml-basics-001"
    }'
  ```

  ```typescript TypeScript
  import Supermemory from 'supermemory'

  const client = new Supermemory({
    apiKey: process.env.SUPERMEMORY_API_KEY
  })

  async function addContent() {
      const result = await client.memories.add({
          content: "Machine learning is a subset of artificial intelligence...",
          containerTags: ["ai-research"],
          metadata: {
            source: "research-notes",
            category: "education",
            priority: "high"
          },
          customId: "ml-basics-001"
        })

        console.log(result) // { id: "abc123", status: "queued" }
  }

   addContent()
  ```

  ```python Python
  from supermemory import Supermemory
  import os

  client = Supermemory(api_key=os.environ.get("SUPERMEMORY_API_KEY"))

  result = client.memories.add(
      content="Machine learning is a subset of artificial intelligence...",
      container_tags=["ai-research"],
      metadata={
          "source": "research-notes",
          "category": "education",
          "priority": "high"
      },
      custom_id="ml-basics-001"
  )

  print(result)  # { "id": "abc123", "status": "queued" }
  ```
</CodeGroup>

<div id="request-parameters">
  #### Paramètres de requête
</div>

| Paramètre | Type | Obligatoire | Description |
|-----------|------|-------------|-------------|
| `content` | string | Oui | Contenu à convertir en document. Peut être du texte, une URL ou d’autres formats pris en charge |
| `containerTag` | string | Non | **Recommandé** : Tag unique pour regrouper des memories liées dans un espace. Par défaut : &quot;sm&#95;project&#95;default&quot; |
| `containerTags` | string[] | Non | Format de tableau hérité. Utilisez plutôt `containerTag` pour de meilleures performances |
| `metadata` | object | Non | Metadata supplémentaires sous forme de paires clé‑valeur (chaînes, nombres, booléens uniquement) |
| `customId` | string | Non | Votre identifiant propre pour ce document (255 caractères max) |
| `raw` | string | Non | Contenu brut à stocker en plus du contenu traité |

<div id="response">
  #### Réponse
</div>

Lorsque vous créez un document avec succès, vous recevez une simple confirmation indiquant l’id du document et son statut de traitement initial :

```json
{
  "id": "D2Ar7Vo7ub83w3PRPZcaP1",
  "status": "queued"
}
```

**Ce que cela signifie :**

* `id` : l’identifiant unique de votre document — conservez-le pour suivre le traitement ou y faire référence plus tard
* `status` : état de traitement actuel. `"queued"` signifie qu’il est en attente d’être transformé en memories

<Note>
  Le document commence à être traité immédiatement en arrière-plan. En quelques secondes à quelques minutes (selon la taille du contenu), il sera découpé en memories interrogeables.
</Note>

<div id="file-upload-drop-and-process">
  ### Téléversement de fichier : glisser-déposer et traitement
</div>

Vous avez un PDF, une image ou une vidéo ? Téléversez-le directement et laissez Supermemory en extraire automatiquement le contenu pertinent.

**Endpoint :** `POST /v3/documents/file`

**Pourquoi c’est puissant :** Plutôt que de copier manuellement du texte depuis des PDF ou de transcrire des vidéos, téléversez simplement le fichier. Supermemory gère l’OCR pour les images, la transcription pour les vidéos et l’extraction intelligente de texte pour les documents.

<CodeGroup>
  ```bash cURL
  curl https://api.supermemory.ai/v3/documents/file \
    -H "Authorization: Bearer $SUPERMEMORY_API_KEY" \
    -F "file=@document.pdf" \
    -F "containerTags=research_project"

  # Réponse :
  # {
  #   "id": "Mx7fK9pL2qR5tE8yU4nC7",
  #   "status": "processing"
  # }
  ```

  ```typescript TypeScript
  import Supermemory from 'supermemory'
  import fs from 'fs'

  const client = new Supermemory({
    apiKey: process.env.SUPERMEMORY_API_KEY
  })

  // Méthode 1 : utiliser la méthode SDK uploadFile (RECOMMANDÉ)
  const result = await client.memories.uploadFile({
    file: fs.createReadStream('/path/to/document.pdf'),
    containerTags: 'research_project'  // Chaîne, pas un tableau !
  })

  // Méthode 2 : utiliser fetch avec des données de formulaire (pour une implémentation navigateur/manuelle)
  const formData = new FormData()
  formData.append('file', fileInput.files[0])
  formData.append('containerTags', 'research_project')

  const response = await fetch('https://api.supermemory.ai/v3/documents/file', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.SUPERMEMORY_API_KEY}`
    },
    body: formData
  })

  const result = await response.json()
  console.log(result)
  // Sortie : { id: "Mx7fK9pL2qR5tE8yU4nC7", status: "processing" }
  ```

  ```python Python
  from supermemory import Supermemory

  client = Supermemory(api_key="your_api_key")

  # Méthode 1 : utiliser la méthode SDK upload_file (RECOMMANDÉ)
  result = client.memories.upload_file(
      file=open('document.pdf', 'rb'),
      container_tags='research_project'  # Paramètre de type chaîne
  )

  # Méthode 2 : utiliser requests avec des données de formulaire
  import requests

  files = {'file': open('document.pdf', 'rb')}
  data = {'containerTags': 'research_project'}

  response = requests.post(
      'https://api.supermemory.ai/v3/documents/file',
      headers={'Authorization': f'Bearer {api_key}'},
      files=files,
      data=data
  )

  result = response.json()
  print(result)
  # Sortie : {'id': 'Mx7fK9pL2qR5tE8yU4nC7', 'status': 'processing'}
  ```
</CodeGroup>

<div id="supported-file-types">
  #### Types de fichiers pris en charge
</div>

<Tabs>
  <Tab title="Documents">
    * **PDF** : Extraction avec prise en charge de l’OCR pour les documents numérisés
    * **Google Docs** : Via l’intégration à l’API Google Drive
    * **Google Sheets** : Extraction du contenu des feuilles de calcul
    * **Google Slides** : Extraction du contenu des présentations
    * **Notion Pages** : Contenu riche avec préservation de la structure en blocs
    * **OneDrive Documents** : Documents Microsoft Office
  </Tab>

  <Tab title="Media">
    * **Images** : JPG, PNG, GIF, WebP avec extraction de texte par OCR
    * **Vidéos** : MP4, WebM, AVI avec transcription (YouTube, Vimeo)
  </Tab>

  <Tab title="Web Content">
    * **Pages web** : Toute URL publique avec extraction intelligente du contenu
    * **Publications Twitter/X** : Contenu des tweets et metadata
    * **Vidéos YouTube** : Transcription automatique et metadata
  </Tab>

  <Tab title="Text Formats">
    * **Texte brut** : Fichiers TXT, MD, CSV
  </Tab>
</Tabs>

<div id="content-types-processing">
  ## Types de contenu et traitement
</div>

<div id="automatic-detection">
  ### Détection automatique
</div>

Supermemory détecte automatiquement les types de contenu en fonction :

* **Motifs d’URL** : analyse du domaine et du chemin pour certains services
* **Types MIME** : détection du type de fichier à partir des en-têtes/Metadata
* **Analyse du contenu** : inspection de la structure et du format
* **Extensions de fichier** : méthode d’identification de repli

```typescript

type MemoryType =
  | 'text'        // Contenu texte brut
  | 'pdf'         // Documents PDF
  | 'tweet'       // Publications Twitter/X
  | 'google_doc'  // Google Docs
  | 'google_slide'// Google Slides
  | 'google_sheet'// Google Sheets
  | 'image'       // Images avec OCR
  | 'video'       // Vidéos avec transcription
  | 'notion_doc'  // Pages Notion
  | 'webpage'     // Pages web
  | 'onedrive'    // Documents OneDrive



// Exemples de détection automatique
const examples = {
  "https://twitter.com/user/status/123": "tweet",
  "https://youtube.com/watch?v=abc": "video",
  "https://docs.google.com/document/d/123": "google_doc",
  "https://docs.google.com/spreadsheets/d/123": "google_sheet",
  "https://docs.google.com/presentation/d/123": "google_slide",
  "https://notion.so/page-123": "notion_doc",
  "https://example.com": "webpage",
  "Regular text content": "text",
  // Fichiers PDF importés → "pdf"
  // Fichiers image importés → "image"
  // Liens OneDrive → "onedrive"
}
```

<div id="processing-pipeline">
  ### Pipeline de traitement
</div>

Chaque type de contenu suit un pipeline de traitement spécialisé :

<Accordion title="Contenu textuel" defaultOpen>
  Le contenu est nettoyé, normalisé et segmenté en chunks pour une recherche optimale :

  1. **En attente** : la memory entre dans la file de traitement
  2. **Extraction** : normalisation et nettoyage du texte
  3. **Segmentation en chunks** : découpage intelligent basé sur la structure du contenu
  4. **Embedding** : conversion en représentations vectorielles pour la recherche
  5. **Indexation** : ajout à l’index de recherche
  6. **Terminé** : extraction des Metadata effectuée
</Accordion>

<Accordion title="Contenu web">
  Les pages web font l’objet d’une extraction de contenu avancée :

  1. **En attente** : url mise en file d’attente pour traitement
  2. **Extraction** : récupération du contenu de la page avec des en-têtes appropriés, suppression de la navigation et des éléments standard, extraction du title, de la description, etc.
  3. **Segmentation en chunks** : contenu découpé pour une recherche optimale
  4. **Embedding** : génération de représentations vectorielles
  5. **Indexation** : ajout à l’index de recherche
  6. **Terminé** : traitement terminé avec `type: 'webpage'`
</Accordion>

<Accordion title="Traitement de fichiers">
  Les fichiers sont traités via des extracteurs spécialisés :

  1. **En attente** : fichier mis en file d’attente pour traitement
  2. **Extraction de contenu** : détection du type et traitement spécifique au format
  3. **OCR/Transcription** : pour les images et les fichiers multimédias
  4. **Segmentation en chunks** : contenu décomposé en segments interrogeables
  5. **Embedding** : création de représentations vectorielles
  6. **Indexation** : ajout à l’index de recherche
  7. **Terminé** : traitement terminé
</Accordion>

<div id="error-handling">
  ## Gestion des erreurs
</div>

<div id="common-errors">
  ### Erreurs courantes
</div>

Faites défiler vers la droite pour en voir plus.

<Tabs>
  <Tab title="Erreurs d’authentification">
    ```json
    // Classe AuthenticationError
    {
      name: "AuthenticationError",
      status: 401,
      message: "401 Unauthorized",
      error: {
        message: "Invalid API key",
        type: "authentication_error"
      }
    }
    ```

    **Causes :**

    * Clé d’API manquante ou invalide
    * Jeton d’authentification expiré
    * Format d’en-tête d’autorisation incorrect
  </Tab>

  <Tab title="Erreurs de requête (400)">
    ```json
    // Classe BadRequestError
    {
      name: "BadRequestError",
      status: 400,
      message: "400 Bad Request",
      error: {
        message: "Invalid request parameters",
        details: {
          content: "Content cannot be empty",
          customId: "customId exceeds maximum length"
        }
      }
    }
    ```

    **Causes :**

    * Champs requis manquants
    * Types de paramètres invalides
    * Contenu trop volumineux
    * ID personnalisé trop long
    * Structure des metadata invalide
  </Tab>

  <Tab title="Limitation de débit (429)">
    ```json
    // Classe RateLimitError
    {
      name: "RateLimitError",
      status: 429,  // PAS 402 !
      message: "429 Too Many Requests",
      error: {
        message: "Rate limit exceeded",
        retry_after: 60
      }
    }
    ```

    **Causes :**

    * Quota mensuel de jetons dépassé
    * Limites de débit dépassées
    * Limites d’abonnement atteintes

    **Correctif :** Implémentez un backoff exponentiel et respectez les limites de débit
  </Tab>

  <Tab title="Erreurs « Not Found » (404)">
    ```json
    // Classe NotFoundError
    {
      name: "NotFoundError",
      status: 404,
      message: "404 Not Found",
      error: {
        message: "Memory not found",
        resource_id: "invalid_memory_id"
      }
    }
    ```

    Causes :

    * L’id de la memory n’existe pas
    * La memory a été supprimée
    * URL d’endpoint invalide
  </Tab>

  <Tab title="Permission refusée (403)">
    ```json
    // Classe PermissionDeniedError
    {
      name: "PermissionDeniedError",
      status: 403,
      message: "403 Forbidden",
      error: {
        message: "Insufficient permissions",
        required_permission: "memories:write"
      }
    }
    ```

    Causes :

    * La clé d’API ne dispose pas des autorisations requises
    * Accès à des ressources restreintes
    * Limitations du compte
  </Tab>

  <Tab title="Erreurs serveur (500+)">
    ```json
    // Classe InternalServerError
    {
      name: "InternalServerError",
      status: 500,
      message: "500 Internal Server Error",
      error: {
        message: "Processing failed",
        details: "Content extraction service unavailable"
      }
    }
    ```

    **Causes :**

    * Service externe indisponible
    * Échec de l’extraction de contenu
  </Tab>

  <Tab title="Erreurs réseau">
    ```json
        // Classe APIConnectionError - NOUVEAU
      {
        name: "APIConnectionError",
        message: "Connection error.",
        cause: Error // Erreur réseau d’origine
      }

      // Classe APIConnectionTimeoutError - NOUVEAU
      {
        name: "APIConnectionTimeoutError",
        message: "Request timed out."
      }
    ```

    Causes :

    * Problèmes de connectivité réseau
    * Échecs de résolution DNS
    * Expiration de la requête
    * Blocage par proxy/pare-feu
  </Tab>
</Tabs>

<div id="best-practices">
  ## Bonnes pratiques
</div>

<div id="container-tags-optimize-for-performance">
  ### Balises de conteneur : optimiser les performances
</div>

Utilisez une seule balise de conteneur pour de meilleures performances de requête. Plusieurs balises sont prises en charge, mais augmentent la latence.

```json
{
  "content": "Flux d’authentification mis à jour pour utiliser des jetons JWT",
  "containerTags": "[project_alpha]",
  "metadata": {
    "type": "modification_technique",
    "author": "sarah_dev",
    "impact": "rupture"
  }
}
```

**Balise unique vs balises multiples**

```javascript
// ✅ Recommandé : balise unique, requêtes plus rapides
{ "containerTags": ["project_alpha"] }

// ⚠️ Autorisé mais plus lent : plusieurs balises augmentent la latence
{ "containerTags": ["project_alpha", "auth", "backend"] }
```

**Pourquoi les tags uniques offrent de meilleures performances :**

* Les memories d’un même espace peuvent se référencer efficacement
* Les requêtes de recherche n’ont pas besoin de parcourir plusieurs espaces
* L’inférence au niveau de l’intégration est plus rapide au sein d’un espace unique

<div id="custom-ids-deduplication-and-updates">
  ### IDs personnalisés : déduplication et mises à jour
</div>

Les IDs personnalisés évitent les doublons et permettent de mettre à jour les documents. Deux méthodes de mise à jour sont disponibles.

**Méthode 1 : POST avec customId (upsert)**

```bash
# Créer un document
curl -X POST "https://api.supermemory.ai/v3/documents" \
  -H "Authorization: Bearer $SUPERMEMORY_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "content": "L’API utilise des endpoints REST",
    "customId": "api_docs_v1",
    "containerTags": ["project_alpha"]
  }'
# Réponse : {"id": "abc123", "status": "queued"}

# Mettre à jour le même document (même customId = upsert)
curl -X POST "https://api.supermemory.ai/v3/documents" \
  -H "Authorization: Bearer $SUPERMEMORY_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "content": "L’API a migré vers GraphQL",
    "customId": "api_docs_v1",
    "containerTags": ["project_alpha"]
  }'
```

**Méthode 2 : PATCH par id (mise à jour)**

```bash
curl -X PATCH "https://api.supermemory.ai/v3/documents/abc123" \
  -H "Authorization: Bearer $SUPERMEMORY_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "content": "L’API utilise désormais GraphQL avec mise en cache",
    "metadata": {"version": 3}
  }'
```

**Schémas d’id personnalisés**

```javascript
// Synchronisation avec des systèmes externes
"jira_PROJ_123"
"confluence_456789"
"github_issue_987"

// Entités de base de données
"user_profile_12345"
"order_67890"

// Contenu versionné
"meeting_2024_01_15"
"api_docs_auth"
"requirements_v3"
```

**Comportement de mise à jour**

* Les anciennes memories sont supprimées
* De nouvelles memories sont créées à partir du contenu mis à jour
* Le même id de document est conservé

<div id="rate-limits-quotas">
  ### Limites de débit et quotas
</div>

**Utilisation des jetons**

```javascript
"Bonjour, monde" // ≈ 2 jetons
"PDF de 10 pages" // ≈ 2 000–4 000 jetons
"Vidéo YouTube (10 min)" // ≈ 1 500–3 000 jetons
"Article web" // ≈ 500–2 000 jetons
```

**Limites actuelles**

| Fonctionnalité | Gratuit | Starter | Growth |
|---------|------|-----|------------|
| Jetons de memory par mois | 100 000 | 1 000 000 | 10 000 000 |
| Requêtes de recherche par mois | 1 000 | 10 000 | 100 000 |

**Réponse en cas de dépassement de limite**

```bash
curl -X POST "https://api.supermemory.ai/v3/documents" \
  -H "Authorization: Bearer your_api_key" \
  -d '{"content": "Du contenu"}'
```

Réponse :

```json
{"error": "Limite de jetons de memory atteinte", "status": 402}
```

<div id="batch-upload-of-documents">
  ## Importation par lots de documents
</div>

Traitez de gros volumes efficacement grâce à une limitation du débit et à une reprise après erreur.

<div id="implementation-strategy">
  ### Stratégie de mise en œuvre
</div>

<Tabs>
  <Tab title="TypeScript">
    ```typescript
    import Supermemory, {
      BadRequestError,
      RateLimitError,
      AuthenticationError
    } from 'supermemory';

    interface Document {
      id: string;
      content: string;
      title?: string;
      createdAt?: string;
      metadata?: Record<string, string | number | boolean>;
    }

    async function batchIngest(documents: Document[], options = {}) {
      const {
        batchSize = 5,
        delayBetweenBatches = 2000,
        maxRetries = 3
      } = options;

      const results = [];

      for (let i = 0; i < documents.length; i += batchSize) {
        const batch = documents.slice(i, i + batchSize);
        console.log(`Traitement du lot ${Math.floor(i/batchSize) + 1}/${Math.ceil(documents.length/batchSize)}`);

        const batchResults = await Promise.allSettled(
          batch.map(doc => ingestWithRetry(doc, maxRetries))
        );

        results.push(...batchResults);

        // Limitation de débit entre les lots
        if (i + batchSize < documents.length) {
          await new Promise(resolve => setTimeout(resolve, delayBetweenBatches));
        }
      }

      return results;
    }

    async function ingestWithRetry(doc: Document, maxRetries: number) {
      for (let attempt = 1; attempt <= maxRetries; attempt++) {
        try {
          return await client.memories.add({
            content: doc.content,
            customId: doc.id,
            containerTags: ["batch_import_user_123"], // CORRIGÉ : tableau
            metadata: {
              source: "migration",
              batch_id: generateBatchId(),
              original_created: doc.createdAt || new Date().toISOString(),
              title: doc.title || "",
              ...doc.metadata
            }
          });
        } catch (error) {
          // CORRIGÉ : gestion des erreurs appropriée
          if (error instanceof AuthenticationError) {
            console.error('Échec de l’authentification — vérifiez la clé d’API');
            throw error; // Ne pas retenter les erreurs d’authentification
          }

          if (error instanceof BadRequestError) {
            console.error('Format de document invalide :', doc.id);
            throw error; // Ne pas retenter les erreurs de validation
          }

          if (error instanceof RateLimitError) {
            console.log(`Limitation de débit à la tentative ${attempt}, attente prolongée...`);
            const delay = Math.pow(2, attempt) * 2000; // Délais plus longs en cas de limitation de débit
            await new Promise(resolve => setTimeout(resolve, delay));
            continue;
          }

          if (attempt === maxRetries) throw error;

          // Repli exponentiel pour les autres erreurs
          const delay = Math.pow(2, attempt) * 1000;
          console.log(`Nouvelle tentative ${attempt}/${maxRetries} pour ${doc.id} dans ${delay} ms`);
          await new Promise(resolve => setTimeout(resolve, delay));
        }
      }
    }

    function generateBatchId(): string {
      return `batch_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
    }
    ```
  </Tab>

  <Tab title="Python">
    ```python
        import asyncio
    import time
    import logging
    from typing import List, Dict, Any, Optional
    from supermemory import Supermemory, BadRequestError, RateLimitError

    async def batch_ingest(
        documents: List[Dict[str, Any]],
        options: Optional[Dict[str, Any]] = None
    ):
        options = options or {}
        batch_size = options.get('batch_size', 5)  # CORRIGÉ : taille prudente
        delay_between_batches = options.get('delay_between_batches', 2.0)  # CORRIGÉ : 2 secondes
        max_retries = options.get('max_retries', 3)

        results = []

        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(documents) + batch_size - 1) // batch_size

            print(f"Traitement du lot {batch_num}/{total_batches}")

            # Traiter le lot avec une gestion des erreurs appropriée
            tasks = [ingest_with_retry(doc, max_retries) for doc in batch]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)

            results.extend(batch_results)

            # Limitation du débit entre les lots
            if i + batch_size < len(documents):
                await asyncio.sleep(delay_between_batches)

        return results

    async def ingest_with_retry(doc: Dict[str, Any], max_retries: int):
        for attempt in range(1, max_retries + 1):
            try:
                return await client.memories.add(
                    content=doc['content'],
                    custom_id=doc['id'],
                    container_tags=["batch_import_user_123"],  # CORRIGÉ : liste
                    metadata={
                        "source": "migration",
                        "batch_id": generate_batch_id(),
                        "original_created": doc.get('created_at', ''),
                        "title": doc.get('title', ''),
                        **doc.get('metadata', {})
                    }
                )
            except BadRequestError as e:
                logging.error(f"Invalid document {doc['id']}: {e}")
                raise  # Ne pas réessayer les erreurs de validation

            except RateLimitError as e:
                logging.warning(f"Limitation du débit à la tentative {attempt}")
                delay = 2 ** attempt * 2  # Délais plus longs en cas de limitation du débit
                await asyncio.sleep(delay)
                continue

            except Exception as error:
                if attempt == max_retries:
                    raise error

                # Repli exponentiel
                delay = 2 ** attempt
                logging.info(f"Nouvelle tentative {attempt}/{max_retries} pour {doc['id']} dans {delay}s")
                await asyncio.sleep(delay)

    def generate_batch_id() -> str:
        import random
        import string
        return f"batch_{int(time.time())}_{random.choices(string.ascii_lowercase, k=8)}"
    ```
  </Tab>
</Tabs>

<div id="best-practices-for-batch-operations">
  ### Bonnes pratiques pour les opérations par lots
</div>

<Accordion title="Optimisation des performances" defaultOpen>
  * **Taille des lots** : 3 à 5 documents à la fois
  * **Délais** : 2 à 3 secondes entre les lots pour éviter le rate limiting
  * **Promise.allSettled()** : Gère les résultats mixtes succès/échec
  * **Suivi de l’avancement** : Surveillez les opérations de longue durée

  **Exemple de sortie**

  ```
  Processing batch 1/50 (documents 1-3)
  Successfully processed: 2/3 documents
  Failed: 1/3 documents (BadRequestError: Invalid content)
  Progress: 3/150 (2.0%) - Next batch in 2s
  ```
</Accordion>

<Accordion title="Gestion des erreurs">
  * **Types d’erreurs spécifiques :** Traitez `BadRequestError`, `RateLimitError`, `AuthenticationError` différemment
  * **Pas de logique de nouvelle tentative** : Ne réessayez pas après des erreurs de validation ou d’authentification
  * **Gestion du rate limit** : Allongez les délais de backoff pour les erreurs de limite de débit
  * **Journalisation** : Enregistrez les échecs pour examen/réexécution
</Accordion>

<Accordion title="Gestion de la mémoire">
  * **Streaming** : Traitez les fichiers volumineux en segments
  * **Nettoyage** : Libérez de la mémoire les lots déjà traités
  * **Persistance de la progression** : Reprenez les migrations interrompues
</Accordion>

<Note>
  Prêt à démarrer l’ingestion ? [Obtenez une clé d’API](https://console.supermemory.ai) dès maintenant !
</Note>