---
title: "摄取 Documents 与数据"
sidebarTitle: "内容摄取指南"
description: "将文本、URL、文件及多种内容类型摄取到 Supermemory 的完整指南"
---

Supermemory 提供功能强大且灵活的摄取系统，几乎可处理任何类型的内容。无论你要添加简单的文本笔记、网页、PDF、图像，还是来自各个平台的复杂文档，我们的 API 都能无缝处理。

<div id="understanding-the-mental-model">
  ## 理解思维模型
</div>

在深入使用 API 之前，先了解 Supermemory 如何处理你的内容非常重要：

<div id="documents-vs-memories">
  ### Documents 与 Memories 的区别
</div>

* **Documents**：你放入 Supermemory 的任何内容（文件、URL、文本）都被视为一个 **document**
* **Memories**：Documents 会被自动切分为更小、可搜索的片段，称为 **memories**

当你使用 “Add Memory” 端点时，实际上是在添加一个 **document**。Supermemory 的任务是智能地将该 document 拆分为便于搜索和检索的最佳 **memories**。

```
你的内容 → 文档 → 处理 → 多条记忆存储
     ↓             ↓           ↓            ↓
   PDF 文件 → 已存文档 → 内容分块 → 可检索的记忆存储
```

你可以在 [Supermemory Console](https://console.supermemory.ai) 中直观查看这一过程，其中的图谱视图会展示你的 Documents 如何被拆分为相互关联的 memory。

<div id="content-sources">
  ### 内容来源
</div>

Supermemory 主要通过三种方式接收内容：

1. **直接 API**：通过 API 端点上传文件或发送内容
2. **连接器**：与 Google Drive、Notion 和 OneDrive 等平台自动集成（[了解连接器详情](/zh/connectors)）
3. **URL 处理**：自动从网页、视频和社交媒体中提取内容

<div id="overview">
  ## 概览
</div>

摄取系统由以下关键组件构成：

* **多种输入方式**：JSON 内容、文件上传和 URL 处理
* **异步处理**：后台工作流负责内容提取与内容分块
* **自动内容检测**：自动识别并处理不同类型的内容
* **空间组织**：使用容器标签将相关记忆存储分组，以提升上下文推断效果
* **状态跟踪**：在整个处理管道中提供实时 status 更新

<div id="how-it-works">
  ### 工作原理
</div>

<Steps>
  <Step title="提交文档">
    发送你的内容（文本、文件或 URL）以创建新文档
  </Step>

  <Step title="验证">
    API 验证请求并检查速率限制/配额
  </Step>

  <Step title="文档存储">
    你的内容会作为文档存储并进入队列等待处理
  </Step>

  <Step title="内容抽取">
    专用提取器会根据文档类型进行处理
  </Step>

  <Step title="Memory 创建">
    文档会被智能切分为多个可搜索的 memory
  </Step>

  <Step title="Embedding 与索引">
    记忆存储会被转换为向量 Embedding（向量表示/嵌入）并加入索引以支持搜索
  </Step>
</Steps>

<div id="ingestion-endpoints">
  ## 采集端点
</div>

<div id="add-document-json-content">
  ### 添加 Document - JSON 内容
</div>

用于添加并处理为 Documents 的内容的主要端点。

**Endpoint：** `POST /v3/documents`

<Note>
  尽管端点名如此，你实际上是在创建一个**document**，Supermemory 会自动将其切分为可搜索的**memories**。
</Note>

<CodeGroup>
  ```bash cURL
  curl https://api.supermemory.ai/v3/documents \
    -H "Authorization: Bearer $SUPERMEMORY_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
      "content": "Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without explicit programming.",
      "containerTags": ["ai-research", "user_123"],
      "metadata": {
        "source": "research-notes",
        "category": "education",
        "priority": "high"
      },
      "customId": "ml-basics-001"
    }'
  ```

  ```typescript TypeScript
  import Supermemory from 'supermemory'

  const client = new Supermemory({
    apiKey: process.env.SUPERMEMORY_API_KEY
  })

  async function addContent() {
      const result = await client.memories.add({
          content: "Machine learning is a subset of artificial intelligence...",
          containerTags: ["ai-research"],
          metadata: {
            source: "research-notes",
            category: "education",
            priority: "high"
          },
          customId: "ml-basics-001"
        })

        console.log(result) // { id: "abc123", status: "queued" }
  }

   addContent()
  ```

  ```python Python
  from supermemory import Supermemory
  import os

  client = Supermemory(api_key=os.environ.get("SUPERMEMORY_API_KEY"))

  result = client.memories.add(
      content="Machine learning is a subset of artificial intelligence...",
      container_tags=["ai-research"],
      metadata={
          "source": "research-notes",
          "category": "education",
          "priority": "high"
      },
      custom_id="ml-basics-001"
  )

  print(result)  # { "id": "abc123", "status": "queued" }
  ```
</CodeGroup>

<div id="request-parameters">
  #### 请求参数
</div>

| 参数 | 类型 | 必填 | 说明 |
|-----------|------|----------|-------------|
| `content` | string | 是 | 要处理为文档的内容。可以是文本、URL 或其他受支持的格式 |
| `containerTag` | string | 否 | **推荐**：用于在一个空间中对相关记忆存储进行分组的单个标签。默认值为 `"sm_project_default"` |
| `containerTags` | string[] | 否 | 旧版数组格式。为获得更高性能，请改用 `containerTag` |
| `metadata` | object | 否 | 额外的键值对 Metadata（仅限字符串、数字、布尔值） |
| `customId` | string | 否 | 你为该文档提供的自定义标识符（最多 255 个字符） |
| `raw` | string | 否 | 与处理后内容一并存储的原始内容 |

<div id="response">
  #### 响应
</div>

成功创建文档后，你会收到一条简单的确认信息，其中包含文档的 id 和其初始处理状态（status）：

```json
{
  "id": "D2Ar7Vo7ub83w3PRPZcaP1",
  "status": "queued"
}
```

**含义如下：**

* `id`：文档的唯一标识符——请保存以便跟踪处理进度或日后引用
* `status`：当前处理状态。`"queued"` 表示正在等待被处理为记忆存储

<Note>
  文档会在后台立即开始处理。几秒至几分钟内（取决于内容大小），它将被切分为可搜索的记忆存储。
</Note>

<div id="file-upload-drop-and-process">
  ### 文件上传：拖拽即可处理
</div>

有 PDF、图片或视频？直接上传，交给 Supermemory 自动提取关键信息。

**Endpoint：** `POST /v3/documents/file`

**优势所在：** 无需手动从 PDF 复制文字或转写视频，只需上传文件。Supermemory 会对图片执行光学字符识别（OCR）、对视频进行转写，并对文档进行智能文本抽取。

<CodeGroup>
  ```bash cURL
  curl https://api.supermemory.ai/v3/documents/file \
    -H "Authorization: Bearer $SUPERMEMORY_API_KEY" \
    -F "file=@document.pdf" \
    -F "containerTags=research_project"

  # Response:
  # {
  #   "id": "Mx7fK9pL2qR5tE8yU4nC7",
  #   "status": "processing"
  # }
  ```

  ```typescript TypeScript
  import Supermemory from 'supermemory'
  import fs from 'fs'

  const client = new Supermemory({
    apiKey: process.env.SUPERMEMORY_API_KEY
  })

  // Method 1: Using SDK uploadFile method (RECOMMENDED)
  const result = await client.memories.uploadFile({
    file: fs.createReadStream('/path/to/document.pdf'),
    containerTags: 'research_project'  // 字符串，不是数组！
  })

  // Method 2: Using fetch with form data (for browser/manual implementation)
  const formData = new FormData()
  formData.append('file', fileInput.files[0])
  formData.append('containerTags', 'research_project')

  const response = await fetch('https://api.supermemory.ai/v3/documents/file', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.SUPERMEMORY_API_KEY}`
    },
    body: formData
  })

  const result = await response.json()
  console.log(result)
  // Output: { id: "Mx7fK9pL2qR5tE8yU4nC7", status: "processing" }
  ```

  ```python Python
  from supermemory import Supermemory

  client = Supermemory(api_key="your_api_key")

  # Method 1: Using SDK upload_file method (RECOMMENDED)
  result = client.memories.upload_file(
      file=open('document.pdf', 'rb'),
      container_tags='research_project'  # 字符串参数名
  )

  # Method 2: Using requests with form data
  import requests

  files = {'file': open('document.pdf', 'rb')}
  data = {'containerTags': 'research_project'}

  response = requests.post(
      'https://api.supermemory.ai/v3/documents/file',
      headers={'Authorization': f'Bearer {api_key}'},
      files=files,
      data=data
  )

  result = response.json()
  print(result)
  # Output: {'id': 'Mx7fK9pL2qR5tE8yU4nC7', 'status': 'processing'}
  ```
</CodeGroup>

<div id="supported-file-types">
  #### 支持的文件类型
</div>

<Tabs>
  <Tab title="Documents">
    * **PDF**: 支持对扫描件进行光学字符识别（OCR）提取
    * **Google Docs**: 通过 Google Drive API 集成
    * **Google Sheets**: 表格内容提取
    * **Google Slides**: 演示文稿内容提取
    * **Notion Pages**: 保留块级结构的富文本内容
    * **OneDrive Documents**: Microsoft Office 文档
  </Tab>

  <Tab title="Media">
    * **Images**: 支持 JPG、PNG、GIF、WebP，并可进行 OCR 文本提取
    * **Videos**: 支持 MP4、WebM、AVI，并可转录（YouTube、Vimeo）
  </Tab>

  <Tab title="Web Content">
    * **Web Pages**: 任何公开 URL，支持智能内容提取
    * **Twitter/X Posts**: 推文内容和 Metadata
    * **YouTube Videos**: 自动转录和 Metadata
  </Tab>

  <Tab title="Text Formats">
    * **Plain Text**: TXT、MD、CSV 文件
  </Tab>
</Tabs>

<div id="content-types-processing">
  ## 内容类型与处理流程
</div>

<div id="automatic-detection">
  ### 自动检测
</div>

Supermemory 会基于以下因素自动识别内容类型：

* **URL 模式**：分析域名和路径以识别特定服务
* **MIME 类型**：从报头/Metadata 获取并判断文件类型
* **内容分析**：检查结构与格式
* **文件扩展名**：作为兜底的识别方式

```typescript

type MemoryType =
  | 'text'        // 纯文本内容
  | 'pdf'         // PDF 文档
  | 'tweet'       // Twitter/X 帖文
  | 'google_doc'  // Google Docs 文档
  | 'google_slide'// Google Slides 幻灯片
  | 'google_sheet'// Google Sheets 表格
  | 'image'       // 含光学字符识别（OCR）的图像
  | 'video'       // 含转录的视频
  | 'notion_doc'  // Notion 页面
  | 'webpage'     // 网页
  | 'onedrive'    // OneDrive 文档



// 自动检测示例
const examples = {
  "https://twitter.com/user/status/123": "tweet",
  "https://youtube.com/watch?v=abc": "video",
  "https://docs.google.com/document/d/123": "google_doc",
  "https://docs.google.com/spreadsheets/d/123": "google_sheet",
  "https://docs.google.com/presentation/d/123": "google_slide",
  "https://notion.so/page-123": "notion_doc",
  "https://example.com": "webpage",
  "Regular text content": "text",
  // 上传的 PDF 文件 → "pdf"
  // 上传的图像文件 → "image"
  // OneDrive 链接 → "onedrive"
}
```

<div id="processing-pipeline">
  ### 处理流水线
</div>

每种内容类型都会遵循专门的处理流水线：

<Accordion title="Text Content" defaultOpen>
  内容会被清理、规范化，并进行内容分块以实现最佳检索：

  1. **Queued**: memory 进入处理队列
  2. **Extracting**: 文本规范化与清理
  3. **Chunking**: 基于内容结构的智能拆分
  4. **Embedding**: 转换为用于搜索的向量表示
  5. **Indexing**: 添加到可搜索索引
  6. **Done:** 完成 Metadata 提取
</Accordion>

<Accordion title="Web Content">
  网页将进行更精细的内容提取：

  1. **Queued:** url 已排队等待处理
  2. **Extracting**: 使用合适的请求头获取页面内容，移除导航与模板内容，提取 title、description 等
  3. **Chunking:** 将内容拆分以实现最佳检索
  4. **Embedding**: 生成向量表示
  5. **Indexing**: 添加到搜索索引
  6. **Done:** 处理完成，带有 `type: 'webpage'`
</Accordion>

<Accordion title="File Processing">
  文件通过专用提取器进行处理：

  1. **Queued**: 文件已排队等待处理
  2. **Content Extraction**: 类型检测与格式特定处理
  3. **OCR/Transcription**: 适用于图像和媒体文件的光学字符识别（OCR）/转录
  4. **Chunking:** 将内容拆分为可搜索的片段
  5. **Embedding:** 创建向量表示
  6. **Indexing:** 添加到搜索索引
  7. **Done:** 处理完成
</Accordion>

<div id="error-handling">
  ## 错误处理
</div>

<div id="common-errors">
  ### 常见错误
</div>

向右滚动以查看更多。

<Tabs>
  <Tab title="Authentication Errors">
    ```json
    // AuthenticationError class
    {
      name: "AuthenticationError",
      status: 401,
      message: "401 Unauthorized",
      error: {
        message: "Invalid API key",
        type: "authentication_error"
      }
    }
    ```

    **原因：**

    * 缺少或无效的 API 密钥
    * 身份验证令牌已过期
    * 授权头格式不正确
  </Tab>

  <Tab title="Bad Request Errors (400)">
    ```json
    // BadRequestError class
    {
      name: "BadRequestError",
      status: 400,
      message: "400 Bad Request",
      error: {
        message: "Invalid request parameters",
        details: {
          content: "Content cannot be empty",
          customId: "customId exceeds maximum length"
        }
      }
    }
    ```

    **原因：**

    * 缺少必填字段
    * 参数类型无效
    * 内容过大
    * 自定义 ID 过长
    * Metadata 结构无效
  </Tab>

  <Tab title="Rate Limiting (429)">
    ```json
    // RateLimitError class
    {
      name: "RateLimitError",
      status: 429,  // NOT 402!
      message: "429 Too Many Requests",
      error: {
        message: "Rate limit exceeded",
        retry_after: 60
      }
    }
    ```

    **原因：**

    * 超出每月令牌配额
    * 触发速率限制
    * 订阅额度用尽

    **修复：** 实现指数退避并遵守速率限制
  </Tab>

  <Tab title="Not Found Errors (404)">
    ```json
    // NotFoundError class
    {
      name: "NotFoundError",
      status: 404,
      message: "404 Not Found",
      error: {
        message: "Memory not found",
        resource_id: "invalid_memory_id"
      }
    }
    ```

    原因：

    * Memory ID 不存在
    * Memory 已被删除
    * 端点 URL 无效
  </Tab>

  <Tab title="Permission Denied (403)">
    ```json
    // PermissionDeniedError class
    {
      name: "PermissionDeniedError",
      status: 403,
      message: "403 Forbidden",
      error: {
        message: "Insufficient permissions",
        required_permission: "memories:write"
      }
    }
    ```

    原因：

    * API 密钥缺少所需权限
    * 访问受限资源
    * 账户权限受限
  </Tab>

  <Tab title="Server Errors (500+)">
    ```json
    // InternalServerError class
    {
      name: "InternalServerError",
      status: 500,
      message: "500 Internal Server Error",
      error: {
        message: "Processing failed",
        details: "Content extraction service unavailable"
      }
    }
    ```

    **原因：**

    * 外部服务不可用
    * 内容提取失败
  </Tab>

  <Tab title="Network Errors">
    ```json
        // APIConnectionError class - NEW
      {
        name: "APIConnectionError",
        message: "Connection error.",
        cause: Error // Original network error
      }

      // APIConnectionTimeoutError class - NEW
      {
        name: "APIConnectionTimeoutError",
        message: "Request timed out."
      }
    ```

    原因：

    * 网络连接问题
    * DNS 解析失败
    * 请求超时
    * 代理/防火墙阻止
  </Tab>
</Tabs>

<div id="best-practices">
  ## 最佳实践
</div>

<div id="container-tags-optimize-for-performance">
  ### 容器标签：优化性能
</div>

尽量使用单个容器标签，以获得更好的查询性能。支持使用多个标签，但会增加 Latency。

```json
{
  "content": "已将身份验证流程更新为使用 JWT 令牌",
  "containerTags": "[project_alpha]",
  "metadata": {
    "type": "technical_change",
    "author": "sarah_dev",
    "impact": "breaking"
  }
}
```

**单标签 vs 多标签**

```javascript
// ✅ 推荐：使用单个标签，查询更快
{ "containerTags": ["project_alpha"] }

// ⚠️ 可行但更慢：使用多个标签会增加 Latency
{ "containerTags": ["project_alpha", "auth", "backend"] }
```

**为什么使用单一标签效果更好：**

* 同一空间中的记忆存储可以高效地相互引用
* 搜索查询无需跨越多个空间
* 在单一空间内进行链接账户推断更快

<div id="custom-ids-deduplication-and-updates">
  ### 自定义 ID：去重与更新
</div>

自定义 ID 可用于防止重复并支持文档更新。有两种更新方式。

**方法一：通过 POST 携带 customId（Upsert）**

```bash
# 创建文档
curl -X POST "https://api.supermemory.ai/v3/documents" \
  -H "Authorization: Bearer $SUPERMEMORY_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "content": "API 使用 REST 端点",
    "customId": "api_docs_v1",
    "containerTags": ["project_alpha"]
  }'
# 响应：{"id": "abc123", "status": "queued"}

# 更新同一文档（相同 customId = upsert）
curl -X POST "https://api.supermemory.ai/v3/documents" \
  -H "Authorization: Bearer $SUPERMEMORY_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "content": "API 已迁移到 GraphQL",
    "customId": "api_docs_v1",
    "containerTags": ["project_alpha"]
  }'
```

**方法二：按 id 进行 PATCH（更新）**

```bash
curl -X PATCH "https://api.supermemory.ai/v3/documents/abc123" \
  -H "Authorization: Bearer $SUPERMEMORY_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "content": "API 现已改用支持缓存的 GraphQL",
    "metadata": {"version": 3}
  }'
```

**自定义 id 规则**

```javascript
// 外部系统同步
"jira_PROJ_123"
"confluence_456789"
"github_issue_987"

// 数据库实体
"user_profile_12345"
"order_67890"

// 版本化内容
"meeting_2024_01_15"
"api_docs_auth"
"requirements_v3"
```

**更新行为**

* 旧的记忆存储将被删除
* 根据更新后的内容创建新的记忆存储
* 保持相同的文档 ID

<div id="rate-limits-quotas">
  ### 速率限制与配额
</div>

**令牌使用**

```javascript
"Hello world" // ≈ 2 个 token
"10 页 PDF" // ≈ 2,000-4,000 个 token
"YouTube 视频（10 分钟）" // ≈ 1,500-3,000 个 token
"网页文章" // ≈ 500-2,000 个 token
```

**当前上限**

| 功能 | Free | Starter | Growth |
|---------|------|-----|------------|
| 每月 memory 令牌配额 | 100,000 | 1,000,000 | 10,000,000 |
| 每月搜索请求数 | 1,000 | 10,000 | 100,000 |

**超出上限时的响应**

```bash
curl -X POST "https://api.supermemory.ai/v3/documents" \
  -H "Authorization: Bearer your_api_key" \
  -d '{"content": "Some content"}'
```

响应：

```json
{"error": "已达到 memory 令牌限制", "status": 402}
```

<div id="batch-upload-of-documents">
  ## 批量上传 Documents
</div>

通过限流与错误恢复，高效处理海量数据。

<div id="implementation-strategy">
  ### 实施方案
</div>

<Tabs>
  <Tab title="TypeScript">
    ```typescript
    import Supermemory, {
      BadRequestError,
      RateLimitError,
      AuthenticationError
    } from 'supermemory';

    interface Document {
      id: string;
      content: string;
      title?: string;
      createdAt?: string;
      metadata?: Record<string, string | number | boolean>;
    }

    async function batchIngest(documents: Document[], options = {}) {
      const {
        batchSize = 5,
        delayBetweenBatches = 2000,
        maxRetries = 3
      } = options;

      const results = [];

      for (let i = 0; i < documents.length; i += batchSize) {
        const batch = documents.slice(i, i + batchSize);
        console.log(`正在处理第 ${Math.floor(i/batchSize) + 1}/${Math.ceil(documents.length/batchSize)} 个批次`);

        const batchResults = await Promise.allSettled(
          batch.map(doc => ingestWithRetry(doc, maxRetries))
        );

        results.push(...batchResults);

        // 在批次之间进行速率限制
        if (i + batchSize < documents.length) {
          await new Promise(resolve => setTimeout(resolve, delayBetweenBatches));
        }
      }

      return results;
    }

    async function ingestWithRetry(doc: Document, maxRetries: number) {
      for (let attempt = 1; attempt <= maxRetries; attempt++) {
        try {
          return await client.memories.add({
            content: doc.content,
            customId: doc.id,
            containerTags: ["batch_import_user_123"], // 已更正：数组
            metadata: {
              source: "migration",
              batch_id: generateBatchId(),
              original_created: doc.createdAt || new Date().toISOString(),
              title: doc.title || "",
              ...doc.metadata
            }
          });
        } catch (error) {
          // 已更正：正确的错误处理
          if (error instanceof AuthenticationError) {
            console.error('身份验证失败——请检查 API 密钥');
            throw error; // Don't retry auth errors
          }

          if (error instanceof BadRequestError) {
            console.error('Invalid document format:', doc.id);
            throw error; // 不要重试验证错误
          }

          if (error instanceof RateLimitError) {
            console.log(`第 ${attempt} 次尝试触发限流，延长等待时间...`);
            const delay = Math.pow(2, attempt) * 2000; // 针对限流采用更长的延迟
            await new Promise(resolve => setTimeout(resolve, delay));
            continue;
          }

          if (attempt === maxRetries) throw error;

          // 其他错误的指数退避
          const delay = Math.pow(2, attempt) * 1000;
          console.log(`将在 ${delay}ms 后重试（第 ${attempt}/${maxRetries} 次）：${doc.id}`);
          await new Promise(resolve => setTimeout(resolve, delay));
        }
      }
    }

    function generateBatchId(): string {
      return `batch_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
    }
    ```
  </Tab>

  <Tab title="Python">
    ```python
        import asyncio
    import time
    import logging
    from typing import List, Dict, Any, Optional
    from supermemory import Supermemory, BadRequestError, RateLimitError

    async def batch_ingest(
        documents: List[Dict[str, Any]],
        options: Optional[Dict[str, Any]] = None
    ):
        options = options or {}
        batch_size = options.get('batch_size', 5)  # 更正：保守的取值
        delay_between_batches = options.get('delay_between_batches', 2.0)  # 更正：2 秒
        max_retries = options.get('max_retries', 3)

        results = []

        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(documents) + batch_size - 1) // batch_size

            print(f"正在处理批次 {batch_num}/{total_batches}")

            # 为批次处理添加完善的错误处理
            tasks = [ingest_with_retry(doc, max_retries) for doc in batch]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)

            results.extend(batch_results)

            # 批次间的限速
            if i + batch_size < len(documents):
                await asyncio.sleep(delay_between_batches)

        return results

    async def ingest_with_retry(doc: Dict[str, Any], max_retries: int):
        for attempt in range(1, max_retries + 1):
            try:
                return await client.memories.add(
                    content=doc['content'],
                    custom_id=doc['id'],
                    container_tags=["batch_import_user_123"],  # 更正：列表
                    metadata={
                        "source": "migration",
                        "batch_id": generate_batch_id(),
                        "original_created": doc.get('created_at', ''),
                        "title": doc.get('title', ''),
                        **doc.get('metadata', {})
                    }
                )
            except BadRequestError as e:
                logging.error(f"Invalid document {doc['id']}: {e}")
                raise  # 验证错误不重试

            except RateLimitError as e:
                logging.warning(f"第 {attempt} 次尝试触发了限速")
                delay = 2 ** attempt * 2  # 针对限速采用更长的延迟
                await asyncio.sleep(delay)
                continue

            except Exception as error:
                if attempt == max_retries:
                    raise error

                # 指数退避
                delay = 2 ** attempt
                logging.info(f"Retry {attempt}/{max_retries} for {doc['id']} in {delay}s")
                await asyncio.sleep(delay)

    def generate_batch_id() -> str:
        import random
        import string
        return f"batch_{int(time.time())}_{random.choices(string.ascii_lowercase, k=8)}"
    ```
  </Tab>
</Tabs>

<div id="best-practices-for-batch-operations">
  ### 批量操作最佳实践
</div>

<Accordion title="性能优化" defaultOpen>
  * **批量大小**：每次处理 3-5 个 Documents
  * **延迟**：批次间隔 2-3 秒可避免触发速率限制
  * **Promise.allSettled()**：可同时处理成功与失败的结果
  * **进度跟踪**：监控长时间运行的任务
    **示例输出**

  ```
  Processing batch 1/50 (documents 1-3)
  Successfully processed: 2/3 documents
  Failed: 1/3 documents (BadRequestError: Invalid content)
  Progress: 3/150 (2.0%) - Next batch in 2s
  ```
</Accordion>

<Accordion title="错误处理">
  * **特定错误类型：**分别处理 `BadRequestError`、`RateLimitError`、`AuthenticationError`
  * **不进行重试逻辑**：不要重试校验或认证类错误
  * **速率限制处理**：对速率限制错误使用更长的退避延迟
  * **日志记录**：记录失败便于审查/重试
</Accordion>

<Accordion title="内存管理">
  * **流式处理**：以 chunk（文档片段）方式处理大文件
  * **清理**：从 memory 中清除已处理的批次
  * **进度持久化**：支持从中断处恢复迁移
</Accordion>

<Note>
  准备开始导入了吗？现在就去[获取 API 密钥](https://console.supermemory.ai)！
</Note>