---
title: "Dokumente und Daten importieren"
sidebarTitle: "Leitfaden zur Inhaltsaufnahme"
description: "Umfassender Leitfaden zum Import von Text, URLs, Dateien und verschiedenen Inhaltstypen in Supermemory"
---

Supermemory bietet ein leistungsfähiges und flexibles Aufnahmesystem, das praktisch jeden Inhaltstyp verarbeiten kann. Ob du einfache Textnotizen, Webseiten, PDFs, Bilder oder komplexe documents aus verschiedenen Plattformen hinzufügst – unsere API verarbeitet alles nahtlos.

<div id="understanding-the-mental-model">
  ## Das mentale Modell verstehen
</div>

Bevor Sie mit der API arbeiten, ist es wichtig zu verstehen, wie Supermemory Ihre Inhalte verarbeitet:

<div id="documents-vs-memories">
  ### Documents vs Speicher
</div>

* **Documents**: Alles, was Sie in Supermemory hochladen (Dateien, URLs, Text), gilt als **document**
* **Speicher**: Documents werden automatisch in kleinere, durchsuchbare Einheiten zerlegt, die **Speicher** genannt werden

Wenn Sie den „Add Memory“-Endpoint verwenden, fügen Sie tatsächlich ein **document** hinzu. Die Aufgabe von Supermemory besteht darin, dieses Document intelligent in optimale **Speicher** zu zerlegen, die durchsucht und abgerufen werden können.

```
Ihr Inhalt → Dokument → Verarbeitung → Mehrere Speicher-Einträge
     ↓             ↓           ↓            ↓
   PDF-Datei → Gespeichertes Dok → Chunking → Durchsuchbare Speicher-Einträge
```

Du kannst diesen Prozess in der [Supermemory Console](https://console.supermemory.ai) visualisieren. Dort siehst du eine Graphansicht, die zeigt, wie deine documents in miteinander verbundene Speicher-Einträge aufgeteilt werden.

<div id="content-sources">
  ### Inhaltsquellen
</div>

Supermemory nimmt Inhalte über drei Hauptmethoden entgegen:

1. **Direkte API**: Dateien hochladen oder Inhalte über API-Endpunkte senden
2. **Connectoren**: Automatisierte Integrationen mit Plattformen wie Google Drive, Notion und OneDrive ([mehr über Connectoren erfahren](/de/connectors))
3. **URL-Verarbeitung**: Automatisches Extrahieren von Webseiten, Videos und sozialen Medien

<div id="overview">
  ## Übersicht
</div>

Das Ingestion‑System besteht aus mehreren Kernkomponenten:

* **Mehrere Eingabemethoden**: JSON‑Inhalte, Datei‑Uploads und URL‑Verarbeitung
* **Asynchrone Verarbeitung**: Hintergrund‑Workflows übernehmen Inhaltsextraktion und Chunking
* **Automatische Inhaltserkennung**: Erkennt und verarbeitet unterschiedliche Inhaltstypen automatisch
* **Space‑Organisation**: Container‑Tags gruppieren verwandte Speicher‑Einträge für bessere Kontextinferenz
* **Statusverfolgung**: Echtzeit‑Status‑Updates in der gesamten Verarbeitungspipeline

<div id="how-it-works">
  ### Funktionsweise
</div>

<Steps>
  <Step title="Dokument übermitteln">
    Übermitteln Sie Ihren Inhalt (Text, Datei oder URL), um ein neues Dokument zu erstellen
  </Step>

  <Step title="Validierung">
    Die API validiert die Anfrage und prüft Rate-Limits/Quoten
  </Step>

  <Step title="Dokumentspeicherung">
    Ihr Inhalt wird als Dokument gespeichert und zur Verarbeitung in die Warteschlange gestellt
  </Step>

  <Step title="Inhaltsextraktion">
    Spezialisierte Extraktoren verarbeiten das Dokument abhängig von seinem Typ
  </Step>

  <Step title="Speichererstellung">
    Das Dokument wird intelligent in mehrere durchsuchbare Speicher-Einträge segmentiert
  </Step>

  <Step title="Embedding & Indexierung">
    Speicher-Einträge werden in Vektor-Embeddings umgewandelt und durchsuchbar gemacht
  </Step>
</Steps>

<div id="ingestion-endpoints">
  ## Ingestion-Endpunkte
</div>

<div id="add-document-json-content">
  ### Dokument hinzufügen – JSON-Inhalt
</div>

Der primäre Endpoint zum Hinzufügen von Inhalten, die zu documents verarbeitet werden.

**Endpoint:** `POST /v3/documents`

<Note>
  Trotz des Endpoint-Namens erstellen Sie ein **document**, das Supermemory automatisch in durchsuchbare **Speicher** zerlegt.
</Note>

<CodeGroup>
  ```bash cURL
  curl https://api.supermemory.ai/v3/documents \
    -H "Authorization: Bearer $SUPERMEMORY_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
      "content": "Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without explicit programming.",
      "containerTags": ["ai-research", "user_123"],
      "metadata": {
        "source": "research-notes",
        "category": "education",
        "priority": "high"
      },
      "customId": "ml-basics-001"
    }'
  ```

  ```typescript TypeScript
  import Supermemory from 'supermemory'

  const client = new Supermemory({
    apiKey: process.env.SUPERMEMORY_API_KEY
  })

  async function addContent() {
      const result = await client.memories.add({
          content: "Machine learning is a subset of artificial intelligence...",
          containerTags: ["ai-research"],
          metadata: {
            source: "research-notes",
            category: "education",
            priority: "high"
          },
          customId: "ml-basics-001"
        })

        console.log(result) // { id: "abc123", status: "queued" }
  }

   addContent()
  ```

  ```python Python
  from supermemory import Supermemory
  import os

  client = Supermemory(api_key=os.environ.get("SUPERMEMORY_API_KEY"))

  result = client.memories.add(
      content="Machine learning is a subset of artificial intelligence...",
      container_tags=["ai-research"],
      metadata={
          "source": "research-notes",
          "category": "education",
          "priority": "high"
      },
      custom_id="ml-basics-001"
  )

  print(result)  # { "id": "abc123", "status": "queued" }
  ```
</CodeGroup>

<div id="request-parameters">
  #### Anfrageparameter
</div>

| Parameter | Typ | Erforderlich | Beschreibung |
|-----------|------|----------|-------------|
| `content` | string | Ja | Der Inhalt, der zu einem Dokument verarbeitet werden soll. Kann Text, eine URL oder andere unterstützte Formate sein |
| `containerTag` | string | Nein | **Empfohlen**: Einzelnes Tag, um verwandte Speicher-Einträge in einem Bereich zu gruppieren. Standard ist „sm&#95;project&#95;default“ |
| `containerTags` | string[] | Nein | Veraltetes Array-Format. Verwenden Sie stattdessen `containerTag` für bessere Performance |
| `metadata` | object | Nein | Zusätzliche Key-Value-Metadata (nur Strings, Zahlen, Booleans) |
| `customId` | string | Nein | Eigene Kennung für dieses Dokument (max. 255 Zeichen) |
| `raw` | string | Nein | Rohinhalt, der zusammen mit dem verarbeiteten Inhalt gespeichert wird |

<div id="response">
  #### Antwort
</div>

Wenn ein Dokument erfolgreich erstellt wurde, erhalten Sie eine einfache Bestätigung mit der Dokument-id und dem anfänglichen Verarbeitungsstatus:

```json
{
  "id": "D2Ar7Vo7ub83w3PRPZcaP1",
  "status": "queued"
}
```

**Was das bedeutet:**

* `id`: Die eindeutige Kennung Ihres Dokuments – speichern Sie sie, um die Verarbeitung nachzuverfolgen oder später darauf zu verweisen
* `status`: Aktueller Verarbeitungszustand. `"queued"` bedeutet, dass es auf die Verarbeitung zu Speicher-Einträgen wartet

<Note>
  Die Verarbeitung des Dokuments startet sofort im Hintergrund. Innerhalb von Sekunden bis Minuten (abhängig von der Inhaltsgröße) wird es in durchsuchbare Speicher-Einträge zerlegt.
</Note>

<div id="file-upload-drop-and-process">
  ### Datei-Upload: Ablegen und verarbeiten
</div>

PDF, Bild oder Video parat? Lade die Datei hoch und lass Supermemory den relevanten Inhalt automatisch extrahieren.

**Endpoint:** `POST /v3/documents/file`

**Warum das stark ist:** Statt Text mühsam aus PDFs zu kopieren oder Videos zu transkribieren, einfach die Datei hochladen. Supermemory übernimmt OCR für Bilder, Transkription für Videos und intelligente Textextraktion für Dokumente.

<CodeGroup>
  ```bash cURL
  curl https://api.supermemory.ai/v3/documents/file \
    -H "Authorization: Bearer $SUPERMEMORY_API_KEY" \
    -F "file=@document.pdf" \
    -F "containerTags=research_project"

  # Response:
  # {
  #   "id": "Mx7fK9pL2qR5tE8yU4nC7",
  #   "status": "processing"
  # }
  ```

  ```typescript TypeScript
  import Supermemory from 'supermemory'
  import fs from 'fs'

  const client = new Supermemory({
    apiKey: process.env.SUPERMEMORY_API_KEY
  })

  // Methode 1: SDK-Methode uploadFile verwenden (EMPFOHLEN)
  const result = await client.memories.uploadFile({
    file: fs.createReadStream('/path/to/document.pdf'),
    containerTags: 'research_project'  // String, kein Array!
  })

  // Methode 2: fetch mit FormData (für Browser/manuelle Implementierung)
  const formData = new FormData()
  formData.append('file', fileInput.files[0])
  formData.append('containerTags', 'research_project')

  const response = await fetch('https://api.supermemory.ai/v3/documents/file', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.SUPERMEMORY_API_KEY}`
    },
    body: formData
  })

  const result = await response.json()
  console.log(result)
  // Output: { id: "Mx7fK9pL2qR5tE8yU4nC7", status: "processing" }
  ```

  ```python Python
  from supermemory import Supermemory

  client = Supermemory(api_key="your_api_key")

  # Methode 1: SDK-Methode upload_file verwenden (EMPFOHLEN)
  result = client.memories.upload_file(
      file=open('document.pdf', 'rb'),
      container_tags='research_project'  # String-Parametername
  )

  # Methode 2: requests mit FormData verwenden
  import requests

  files = {'file': open('document.pdf', 'rb')}
  data = {'containerTags': 'research_project'}

  response = requests.post(
      'https://api.supermemory.ai/v3/documents/file',
      headers={'Authorization': f'Bearer {api_key}'},
      files=files,
      data=data
  )

  result = response.json()
  print(result)
  # Output: {'id': 'Mx7fK9pL2qR5tE8yU4nC7', 'status': 'processing'}
  ```
</CodeGroup>

<div id="supported-file-types">
  #### Unterstützte Dateitypen
</div>

<Tabs>
  <Tab title="Dokumente">
    * **PDF**: Extraktion mit OCR-Unterstützung für gescannte Dokumente
    * **Google Docs**: Über die Google Drive API-Integration
    * **Google Sheets**: Extraktion von Tabellenkalkulationsinhalten
    * **Google Slides**: Extraktion von Präsentationsinhalten
    * **Notion-Seiten**: Umfangreiche Inhalte mit Erhalt der Blockstruktur
    * **OneDrive-Dokumente**: Microsoft-Office-Dokumente
  </Tab>

  <Tab title="Medien">
    * **Bilder**: JPG, PNG, GIF, WebP mit OCR-Texterkennung
    * **Videos**: MP4, WebM, AVI mit Transkription (YouTube, Vimeo)
  </Tab>

  <Tab title="Webinhalte">
    * **Webseiten**: Beliebige öffentliche URLs mit intelligenter Inhaltsextraktion
    * **Twitter/X-Posts**: Tweet-Inhalte und Metadata
    * **YouTube-Videos**: Automatische Transkription und Metadata
  </Tab>

  <Tab title="Textformate">
    * **Nur-Text**: TXT-, MD- und CSV-Dateien
  </Tab>
</Tabs>

<div id="content-types-processing">
  ## Inhaltstypen und Verarbeitung
</div>

<div id="automatic-detection">
  ### Automatische Erkennung
</div>

Supermemory erkennt Inhaltstypen automatisch anhand von:

* **URL‑Mustern**: Analyse von Domain und Pfad für spezielle Dienste
* **MIME‑Typen**: Erkennung des Dateityps aus Headern/Metadata
* **Inhaltsanalyse**: Untersuchung von Struktur und Format
* **Dateiendungen**: Fallback‑Methode zur Identifizierung

```typescript

type MemoryType =
  | 'text'        // Reiner Textinhalt
  | 'pdf'         // PDF-Dokumente
  | 'tweet'       // Twitter/X-Posts
  | 'google_doc'  // Google Docs
  | 'google_slide'// Google Slides
  | 'google_sheet'// Google Sheets
  | 'image'       // Bilder mit OCR (Optische Zeichenerkennung)
  | 'video'       // Videos mit Transkription
  | 'notion_doc'  // Notion-Seiten
  | 'webpage'     // Webseiten
  | 'onedrive'    // OneDrive-Dokumente



// Beispiele für automatische Erkennung
const examples = {
  "https://twitter.com/user/status/123": "tweet",
  "https://youtube.com/watch?v=abc": "video",
  "https://docs.google.com/document/d/123": "google_doc",
  "https://docs.google.com/spreadsheets/d/123": "google_sheet",
  "https://docs.google.com/presentation/d/123": "google_slide",
  "https://notion.so/page-123": "notion_doc",
  "https://example.com": "webpage",
  "Regular text content": "text",
  // PDF-Dateien hochgeladen → "pdf"
  // Bilddateien hochgeladen → "image"
  // OneDrive-Links → "onedrive"
}
```

<div id="processing-pipeline">
  ### Verarbeitungs-Pipeline
</div>

Jeder Inhaltstyp durchläuft eine spezialisierte Verarbeitungs-Pipeline:

<Accordion title="Textinhalt" defaultOpen>
  Der Inhalt wird bereinigt, normalisiert und für optimales Retrieval in Chunks aufgeteilt:

  1. **In Warteschlange**: Speicher wird in die Verarbeitung eingereiht
  2. **Extraktion**: Textnormalisierung und Bereinigung
  3. **Chunking**: Intelligente Aufteilung anhand der Inhaltsstruktur
  4. **Embedding**: Umwandlung in Vektorrepräsentationen für die Suche
  5. **Indexierung**: Hinzufügen zum durchsuchbaren Index
  6. **Fertig:** Metadata-Extraktion abgeschlossen
</Accordion>

<Accordion title="Webinhalt">
  Webseiten durchlaufen eine fortgeschrittene Inhaltsextraktion:

  1. **In Warteschlange:** url zur Verarbeitung eingereiht
  2. **Extraktion**: Seiteninhalt mit passenden Headern abrufen, Navigation und Boilerplate entfernen, title, Beschreibung usw. extrahieren
  3. **Chunking:** Inhalt für optimales Retrieval aufgeteilt
  4. **Embedding**: Erstellung der Vektorrepräsentation
  5. **Indexierung**: Hinzufügen zum Suchindex
  6. **Fertig:** Verarbeitung abgeschlossen mit `type: 'webpage'`
</Accordion>

<Accordion title="Dateiverarbeitung">
  Dateien werden mit spezialisierten Extraktoren verarbeitet:

  1. **In Warteschlange**: Datei zur Verarbeitung eingereiht
  2. **Inhaltsextraktion**: Typenerkennung und formatspezifische Verarbeitung
  3. **OCR/Transkription**: Für Bild- und Mediendateien
  4. **Chunking:** Inhalt in durchsuchbare Segmente aufgeteilt
  5. **Embedding:** Erstellung der Vektorrepräsentation
  6. **Indexierung:** Hinzufügen zum Suchindex
  7. **Fertig:** Verarbeitung abgeschlossen
</Accordion>

<div id="error-handling">
  ## Fehlerbehandlung
</div>

<div id="common-errors">
  ### Häufige Fehler
</div>

Nach rechts scrollen, um mehr zu sehen.

<Tabs>
  <Tab title="Authentication Errors">
    ```json
    // AuthenticationError class
    {
      name: "AuthenticationError",
      status: 401,
      message: "401 Unauthorized",
      error: {
        message: "Invalid API key",
        type: "authentication_error"
      }
    }
    ```

    **Ursachen:**

    * Fehlender oder ungültiger API-Schlüssel
    * Abgelaufenes Authentifizierungs-Token
    * Falsches Format des Authorization-Headers
  </Tab>

  <Tab title="Bad Request Errors (400)">
    ```json
    // BadRequestError class
    {
      name: "BadRequestError",
      status: 400,
      message: "400 Bad Request",
      error: {
        message: "Invalid request parameters",
        details: {
          content: "Content cannot be empty",
          customId: "customId exceeds maximum length"
        }
      }
    }
    ```

    **Ursachen:**

    * Erforderliche Felder fehlen
    * Ungültige Parametertypen
    * Inhalt zu groß
    * customId zu lang
    * Ungültige Metadata-Struktur
  </Tab>

  <Tab title="Rate Limiting (429)">
    ```json
    // RateLimitError class
    {
      name: "RateLimitError",
      status: 429,  // NOT 402!
      message: "429 Too Many Requests",
      error: {
        message: "Rate limit exceeded",
        retry_after: 60
      }
    }
    ```

    **Ursachen:**

    * Monatliches Token-Kontingent überschritten
    * Rate Limits überschritten
    * Abonnementlimits erreicht

    **Lösung:** Exponentielles Backoff implementieren und Rate Limits einhalten
  </Tab>

  <Tab title="Not Found Errors (404)">
    ```json
    // NotFoundError class
    {
      name: "NotFoundError",
      status: 404,
      message: "404 Not Found",
      error: {
        message: "Memory not found",
        resource_id: "invalid_memory_id"
      }
    }
    ```

    Ursachen:

    * Memory-ID existiert nicht
    * Memory wurde gelöscht
    * Ungültige Endpoint-URL
  </Tab>

  <Tab title="Permission Denied (403)">
    ```json
    // PermissionDeniedError class
    {
      name: "PermissionDeniedError",
      status: 403,
      message: "403 Forbidden",
      error: {
        message: "Insufficient permissions",
        required_permission: "memories:write"
      }
    }
    ```

    Ursachen:

    * API-Schlüssel hat nicht die erforderlichen Berechtigungen
    * Zugriff auf eingeschränkte Ressourcen
    * Kontobeschränkungen
  </Tab>

  <Tab title="Server Errors (500+)">
    ```json
    // InternalServerError class
    {
      name: "InternalServerError",
      status: 500,
      message: "500 Internal Server Error",
      error: {
        message: "Processing failed",
        details: "Content extraction service unavailable"
      }
    }
    ```

    **Ursachen:**

    * Externer Dienst nicht verfügbar
    * Fehler bei der Inhaltsextraktion
  </Tab>

  <Tab title="Network Errors">
    ```json
        // APIConnectionError class - NEW
      {
        name: "APIConnectionError",
        message: "Connection error.",
        cause: Error // Original network error
      }

      // APIConnectionTimeoutError class - NEW
      {
        name: "APIConnectionTimeoutError",
        message: "Request timed out."
      }
    ```

    Ursachen:

    * Netzwerkverbindungsprobleme
    * DNS-Auflösungsfehler
    * Zeitüberschreitungen bei Anfragen
    * Proxy/Firewall-Blockierung
  </Tab>
</Tabs>

<div id="best-practices">
  ## Best Practices
</div>

<div id="container-tags-optimize-for-performance">
  ### Container-Tags: Für maximale Performance optimieren
</div>

Verwenden Sie einzelne Container-Tags für bessere Abfrage-Performance. Mehrere Tags werden zwar unterstützt, erhöhen jedoch die Latenz.

```json
{
  "content": "Authentifizierungsablauf aktualisiert, um JWT-Token zu verwenden",
  "containerTags": "[project_alpha]",
  "metadata": {
    "type": "technical_change",
    "author": "sarah_dev",
    "impact": "breaking"
  }
}
```

**Einzelne vs. mehrere Tags**

```javascript
// ✅ Empfohlen: Einzelner Tag, schnellere Abfragen
{ "containerTags": ["project_alpha"] }

// ⚠️ Erlaubt, aber langsamer: Mehrere Tags erhöhen die Latenz
{ "containerTags": ["project_alpha", "auth", "backend"] }
```

**Warum einzelne Tags besser funktionieren:**

* Speicher-Einträge im selben Space können effizient aufeinander verweisen
* Suchanfragen müssen nicht mehrere Spaces durchlaufen
* Verknüpfungsinferenz ist innerhalb eines einzelnen Space schneller

<div id="custom-ids-deduplication-and-updates">
  ### Custom IDs: Deduplizierung und Aktualisierungen
</div>

Custom IDs verhindern Duplikate und ermöglichen Dokumentaktualisierungen. Zwei Aktualisierungsmethoden sind verfügbar.

**Methode 1: POST mit customId (Upsert)**

```bash
# Dokument erstellen
curl -X POST "https://api.supermemory.ai/v3/documents" \
  -H "Authorization: Bearer $SUPERMEMORY_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "content": "API verwendet REST-Endpunkte",
    "customId": "api_docs_v1",
    "containerTags": ["project_alpha"]
  }'
# Antwort: {"id": "abc123", "status": "queued"}

# Dasselbe Dokument aktualisieren (gleiche customId = upsert)
curl -X POST "https://api.supermemory.ai/v3/documents" \
  -H "Authorization: Bearer $SUPERMEMORY_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "content": "API zu GraphQL migriert",
    "customId": "api_docs_v1",
    "containerTags": ["project_alpha"]
  }'
```

**Methode 2: PATCH per id (Update)**

```bash
curl -X PATCH "https://api.supermemory.ai/v3/documents/abc123" \
  -H "Authorization: Bearer $SUPERMEMORY_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "content": "API nutzt jetzt GraphQL mit Caching",
    "metadata": {"version": 3}
  }'
```

**Eigene ID‑Muster**

```javascript
// Synchronisation externer Systeme
"jira_PROJ_123"
"confluence_456789"
"github_issue_987"

// Datenbankentitäten
"user_profile_12345"
"order_67890"

// Versionierte Inhalte
"meeting_2024_01_15"
"api_docs_auth"
"requirements_v3"
```

**Aktualisierungsverhalten**

* Alte Speicher-Einträge werden gelöscht
* Aus aktualisierten Inhalten werden neue Speicher-Einträge erstellt
* Die gleiche Dokument-ID bleibt erhalten

### Ratenlimits &amp; Kontingente

**Tokenverbrauch**

```javascript
„Hallo Welt" // ≈ 2 tokens
„10-seitiges PDF" // ≈ 2.000-4.000 tokens
„YouTube-Video (10 Min.)" // ≈ 1.500-3.000 tokens
„Web-Artikel" // ≈ 500-2.000 tokens
```

**Aktuelle Grenzwerte**

| Funktion | Free | Starter | Growth |
|---------|------|-----|------------|
| Speicher‑Tokens pro Monat | 100.000 | 1.000.000 | 10.000.000 |
| Suchanfragen pro Monat | 1.000 | 10.000 | 100.000 |

**Antwort bei Überschreiten des Grenzwerts**

```bash
curl -X POST "https://api.supermemory.ai/v3/documents" \
  -H "Authorization: Bearer your_api_key" \
  -d '{"content": "Some content"}'
```

Antwort:

```json
{"error": "Speicher-Token-Limit erreicht", "status": 402}
```

<div id="batch-upload-of-documents">
  ## Batch-Upload von documents
</div>

Verarbeite große Datenmengen effizient mit Rate Limiting und Fehlerbehandlung.

<div id="implementation-strategy">
  ### Umsetzungsstrategie
</div>

<Tabs>
  <Tab title="TypeScript">
    ```typescript
    import Supermemory, {
      BadRequestError,
      RateLimitError,
      AuthenticationError
    } from 'supermemory';

    interface Document {
      id: string;
      content: string;
      title?: string;
      createdAt?: string;
      metadata?: Record<string, string | number | boolean>;
    }

    async function batchIngest(documents: Document[], options = {}) {
      const {
        batchSize = 5,
        delayBetweenBatches = 2000,
        maxRetries = 3
      } = options;

      const results = [];

      for (let i = 0; i < documents.length; i += batchSize) {
        const batch = documents.slice(i, i + batchSize);
        console.log(`Verarbeite Batch ${Math.floor(i/batchSize) + 1}/${Math.ceil(documents.length/batchSize)}`);

        const batchResults = await Promise.allSettled(
          batch.map(doc => ingestWithRetry(doc, maxRetries))
        );

        results.push(...batchResults);

        // Rate-Limiting zwischen Batches
        if (i + batchSize < documents.length) {
          await new Promise(resolve => setTimeout(resolve, delayBetweenBatches));
        }
      }

      return results;
    }

    async function ingestWithRetry(doc: Document, maxRetries: number) {
      for (let attempt = 1; attempt <= maxRetries; attempt++) {
        try {
          return await client.memories.add({
            content: doc.content,
            customId: doc.id,
            containerTags: ["batch_import_user_123"], // KORRIGIERT: Array
            metadata: {
              source: "migration",
              batch_id: generateBatchId(),
              original_created: doc.createdAt || new Date().toISOString(),
              title: doc.title || "",
              ...doc.metadata
            }
          });
        } catch (error) {
          // KORRIGIERT: Ordnungsgemäße Fehlerbehandlung
          if (error instanceof AuthenticationError) {
            console.error('Authentifizierung fehlgeschlagen - API-Schlüssel überprüfen');
            throw error; // Authentifizierungsfehler nicht wiederholen
          }

          if (error instanceof BadRequestError) {
            console.error('Ungültiges Dokumentformat:', doc.id);
            throw error; // Validierungsfehler nicht wiederholen
          }

          if (error instanceof RateLimitError) {
            console.log(`Rate-Limit bei Versuch ${attempt} erreicht, warte länger...`);
            const delay = Math.pow(2, attempt) * 2000; // Längere Verzögerungen für Rate-Limits
            await new Promise(resolve => setTimeout(resolve, delay));
            continue;
          }

          if (attempt === maxRetries) throw error;

          // Exponentieller Backoff für andere Fehler
          const delay = Math.pow(2, attempt) * 1000;
          console.log(`Wiederholung ${attempt}/${maxRetries} für ${doc.id} in ${delay}ms`);
          await new Promise(resolve => setTimeout(resolve, delay));
        }
      }
    }

    function generateBatchId(): string {
      return `batch_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
    }
    ```
  </Tab>

  <Tab title="Python">
    ```python
        import asyncio
    import time
    import logging
    from typing import List, Dict, Any, Optional
    from supermemory import Supermemory, BadRequestError, RateLimitError

    async def batch_ingest(
        documents: List[Dict[str, Any]],
        options: Optional[Dict[str, Any]] = None
    ):
        options = options or {}
        batch_size = options.get('batch_size', 5)  # KORRIGIERT: Konservative Größe
        delay_between_batches = options.get('delay_between_batches', 2.0)  # KORRIGIERT: 2 Sekunden
        max_retries = options.get('max_retries', 3)

        results = []

        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(documents) + batch_size - 1) // batch_size

            print(f"Verarbeite Batch {batch_num}/{total_batches}")

            # Batch mit ordnungsgemäßer Fehlerbehandlung verarbeiten
            tasks = [ingest_with_retry(doc, max_retries) for doc in batch]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)

            results.extend(batch_results)

            # Rate Limiting zwischen Batches
            if i + batch_size < len(documents):
                await asyncio.sleep(delay_between_batches)

        return results

    async def ingest_with_retry(doc: Dict[str, Any], max_retries: int):
        for attempt in range(1, max_retries + 1):
            try:
                return await client.memories.add(
                    content=doc['content'],
                    custom_id=doc['id'],
                    container_tags=["batch_import_user_123"],  # KORRIGIERT: Liste
                    metadata={
                        "source": "migration",
                        "batch_id": generate_batch_id(),
                        "original_created": doc.get('created_at', ''),
                        "title": doc.get('title', ''),
                        **doc.get('metadata', {})
                    }
                )
            except BadRequestError as e:
                logging.error(f"Ungültiges Dokument {doc['id']}: {e}")
                raise  # Validierungsfehler nicht wiederholen

            except RateLimitError as e:
                logging.warning(f"Rate Limit erreicht bei Versuch {attempt}")
                delay = 2 ** attempt * 2  # Längere Verzögerungen für Rate Limits
                await asyncio.sleep(delay)
                continue

            except Exception as error:
                if attempt == max_retries:
                    raise error

                # Exponentieller Backoff
                delay = 2 ** attempt
                logging.info(f"Wiederholung {attempt}/{max_retries} für {doc['id']} in {delay}s")
                await asyncio.sleep(delay)

    def generate_batch_id() -> str:
        import random
        import string
        return f"batch_{int(time.time())}_{random.choices(string.ascii_lowercase, k=8)}"
    ```
  </Tab>
</Tabs>

<div id="best-practices-for-batch-operations">
  ### Best Practices für Batch-Operationen
</div>

<Accordion title="Leistungsoptimierung" defaultOpen>
  * **Batch-Größe**: 3–5 documents auf einmal
  * **Verzögerungen**: 2–3 Sekunden zwischen Batches verhindern Rate Limits
  * **Promise.allSettled()**: Verarbeitet gemischte Erfolgs-/Fehlschläge
  * **Fortschrittsverfolgung**: Langlaufende Operationen überwachen

  **Beispielausgabe**

  ```
  Processing batch 1/50 (documents 1-3)
  Successfully processed: 2/3 documents
  Failed: 1/3 documents (BadRequestError: Invalid content)
  Progress: 3/150 (2.0%) - Next batch in 2s
  ```
</Accordion>

<Accordion title="Fehlerbehandlung">
  * **Spezifische Fehlertypen:** `BadRequestError`, `RateLimitError`, `AuthenticationError` unterschiedlich behandeln
  * **Kein Retry-Mechanismus**: Validierungs- oder Auth-Fehler nicht erneut versuchen
  * **Umgang mit Rate Limits**: Längere Backoff-Verzögerungen bei Rate-Limit-Fehlern
  * **Logging**: Fehler für Prüfung/erneuten Versuch protokollieren
</Accordion>

<Accordion title="Speicherverwaltung">
  * **Streaming**: Große Dateien in Chunks verarbeiten
  * **Bereinigung**: Verarbeitete Batches aus dem Speicher entfernen
  * **Fortschrittspersistenz**: Unterbrochene Migrationen fortsetzen
</Accordion>

<Note>
  Bereit mit dem Ingest starten? [Hole dir einen API-Schlüssel](https://console.supermemory.ai)!
</Note>